<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>江的个人博客</title>
  
  <subtitle>江</subtitle>
  <link href="http://kjiang.com/atom.xml" rel="self"/>
  
  <link href="http://kjiang.com/"/>
  <updated>2025-03-15T04:42:40.880Z</updated>
  <id>http://kjiang.com/</id>
  
  <author>
    <name>jiang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2.k8s-kubeadm安装</title>
    <link href="http://kjiang.com/posts/3480299370.html"/>
    <id>http://kjiang.com/posts/3480299370.html</id>
    <published>2025-03-15T04:15:17.000Z</published>
    <updated>2025-03-15T04:42:40.880Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、kubeadm-安装多-master-节点的-k8s-集群-1-20-以上稳定版本"><a href="#一、kubeadm-安装多-master-节点的-k8s-集群-1-20-以上稳定版本" class="headerlink" title="一、kubeadm 安装多 master 节点的 k8s 集群-1.20 以上稳定版本"></a>一、<strong>kubeadm 安装多 master 节点的 k8s 集群-1.20</strong> 以上稳定版本</h1><p><strong>k8s 环境规划：</strong></p><p> <strong>podSubnet（pod 网段） 10.244.0.0&#x2F;16</strong> </p><p> <strong>serviceSubnet（service 网段）: 10.10.0.0&#x2F;16</strong> </p><p><strong>实验环境规划：</strong></p><p><strong>操作系统：centos7.6</strong></p><p><strong>配置： 4Gib 内存&#x2F;6vCPU&#x2F;100G 硬盘</strong></p><p><strong>网络：NAT</strong></p><p><strong>开启虚拟机的虚拟化</strong></p><p>主节点：192.168.162.180</p><p>主节点：192.168.162.181</p><p>工作节点：192.168.162.182</p><p>VIP：192.168.162.199</p><table><thead><tr><th>K8S集群角色</th><th align="left">Ip</th><th align="left">主机名</th><th>安装的组件</th></tr></thead><tbody><tr><td>控制节点</td><td align="left">192.168.162.180</td><td align="left">master1</td><td>apiserver、controller-manager、scheduler、kubelet、etcd、docker、kube-proxy、calico、keepalived、nginx</td></tr><tr><td>控制节点</td><td align="left">192.168.162.181</td><td align="left">master2</td><td>apiserver、controller-manager、scheduler、kubelet、etcd、docker、kube-proxy、calico、keepalived、nginx</td></tr><tr><td>工作节点</td><td align="left">192.168.162.182</td><td align="left">node1</td><td>kubelet、kube-proxy、docker、calico、coredns</td></tr><tr><td>Vip</td><td align="left">192.168.162.199</td><td align="left"></td><td></td></tr></tbody></table><p><strong>kubeadm 和二进制安装 k8s 适用场景分析</strong></p><p>​kubeadm 是官方提供的开源工具，是一个开源项目，用于快速搭建 kubernetes 集群，目前是比较方便和推荐使用的。kubeadm init 以及 kubeadm join 这两个命令可以快速创建 kubernetes 集群。Kubeadm 初始化 k8s，所有的组件都是以 pod 形式运行的，具备故障自恢复能力。 kubeadm 是工具，可以快速搭建集群，也就是相当于用程序脚本帮我们装好了集群，属于自动部署，简化部署操作，自动部署屏蔽了很多细节，使得对各个模块感知很少，如果对 k8s 架构组件理解不深的话，遇到问题比较难排查。 </p><p><strong>kubeadm</strong> 适合需要经常部署 k8s，或者对自动化要求比较高的场景下使用。 </p><p>**二进制：**在官网下载相关组件的二进制包，如果手动安装，对 kubernetes 理解也会更全面。 </p><p>Kubeadm 和二进制都适合生产环境，在生产环境运行都很稳定，具体如何选择，可以根据实际项目</p><p>进行评估。 </p><p><strong>缺点：</strong> kubeadm部署的k8s集群是单节点的etcd，同时，kubeadm通过容器方式启动，相比二进制安装后启动要慢，而且重启后有可能启动不起来，如果不熟悉k8s，很难排查问题。</p><h1 id="二、初始化基础环境（所有节点执行）"><a href="#二、初始化基础环境（所有节点执行）" class="headerlink" title="二、初始化基础环境（所有节点执行）"></a>二、初始化基础环境（所有节点执行）</h1><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext"># 所有节点执行# 1.1 修改机器 IP，变成静态 IP vim /etc/sysconfig/network-scripts/ifcfg-ens32TYPE=Ethernet PROXY_METHOD=none BROWSER_ONLY=no BOOTPROTO=static IPADDR=192.168.162.180 NETMASK=255.255.255.0 GATEWAY=192.168.162.2 DNS1=192.168.162.2 DEFROUTE=yes IPV4_FAILURE_FATAL=no IPV6INIT=yes IPV6_AUTOCONF=yes IPV6_DEFROUTE=yes IPV6_FAILURE_FATAL=no IPV6_ADDR_GEN_MODE=stable-privacy NAME=ens32DEVICE=ens32ONBOOT=yes # 重启网络service network restart# 1.2 配置机器主机名 # 在 192.168.162.180 上执行如下： hostnamectl set-hostname master1 && bash # 在 192.168.162.181 上执行如下： hostnamectl set-hostname master2 && bash # 在 192.168.162.182 上执行如下： hostnamectl set-hostname node1 && bash  # 1.3 配置主机 hosts 文件，相互之间通过主机名互相访问 修改每台机器的/etc/hosts 文件，增加如下三行： 192.168.162.180 master1 192.168.162.181 master2 192.168.162.182 node1  # 1.4 配置主机之间无密码登录 ssh-keygen #一路回车，不输入密码 # 把本地生成的密钥文件和私钥文件拷贝到远程主机 for sshId in master1 master2 node1 ; do    ssh-copy-id $sshIddone# 1.5 关闭交换分区 swap，提升性能 #临时关闭swapoff -a #永久关闭：注释 swap 挂载，给 swap 这行开头加一下注释sed -ri &#x27;s/.*swap.*/#&/&#x27; /etc/fstab #如果是克隆的虚拟机，需要删除 UUID # 1.6 修改机器内核参数 modprobe br_netfilter echo "modprobe br_netfilter" >> /etc/profile cat > /etc/sysctl.d/k8s.conf << EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1EOF# 更新sysctl -p /etc/sysctl.d/k8s.conf #1.7 关闭 firewalld 防火墙 systemctl stop firewalld ; systemctl disable firewalld  #1.7 关闭 selinux sed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27; /etc/selinux/config #修改 selinux 配置文件之后，重启机器，selinux 配置才能永久生效getenforce #显示 Disabled 说明 selinux 已经关闭# 1.8 配置阿里云的 repo 源  在 master1 上操作： 安装 rzsz,scp命令 yum install lrzsz openssh-clients  -y  #备份基础 repo 源 mkdir /root/repo.bak cd /etc/yum.repos.d/ mv * /root/repo.bak/ #下载阿里云的 repo 源 ,或者上传进来wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo#配置国内阿里云 docker 的 repo 源 yum install yum-utils -yyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # 1.9 配置安装 k8s 组件需要的阿里云的 repo 源 cat <<EOF > /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF# 更新yum源yum clean all && yum makecache# 1.10 配置时间同步 #安装 ntpdate 命令yum install ntpdate -y #跟网络时间做同步ntpdate cn.pool.ntp.org #把时间同步做成计划任务crontab -e * */1 * * * /usr/sbin/ntpdate cn.pool.ntp.org #重启 crond 服务service crond restart 1.11 开启 ipvs ,把 ipvs.modules 上传到各个机器的/etc/sysconfig/modules/目录下cd /etc/sysconfig/modules/# 上传到机器的/etc/sysconfig/modules/目录下scp /etc/sysconfig/modules/ipvs.modules master2:/etc/sysconfig/modules/ scp /etc/sysconfig/modules/ipvs.modules node1:/etc/sysconfig/modules/ chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep ip_vs # 显示如下ip_vs_ftp 13079 0 nf_nat 26583 1 ip_vs_ftp ip_vs_sed 12519 0 ip_vs_nq 12516 0 ip_vs_sh 12688 0 ip_vs_dh 12688 0 # 1.12 安装基础软件包 yum install -y yum-utils device-mapper-persistent-data lvm2 wget net-tools nfs-utils lrzsz gcc gcc-c++ make cmake libxml2-devel openssl-devel curl curl-devel unzip sudo ntp libaio-devel wget vim ncurses-devel autoconf automake zlib-devel python-devel epel-release openssh-server socat ipvsadm conntrack ntpdate telnet ipvsadm # 1.13 安装 iptables # 如果用 firewalld 不习惯，可以安装 iptables ，在master1、master2、node1 上操作：#安装 iptablesyum install iptables-services -y#禁用 iptablesservice iptables stop && systemctl disable iptables#清空防火墙规则iptables -F<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="三、安装docker（所有节点执行）"><a href="#三、安装docker（所有节点执行）" class="headerlink" title="三、安装docker（所有节点执行）"></a>三、安装docker（所有节点执行）</h1><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">2、安装 docker 服务，所有节点执行2.1 安装 docker-ce yum install docker-ce-20.10.6 docker-ce-cli-20.10.6 containerd.io -y systemctl start docker && systemctl enable docker && systemctl status docker  2.2 配置 docker 镜像加速器和驱动 vim /etc/docker/daemon.json &#123; "registry-mirrors":["https://rsbud4vc.mirror.aliyuncs.com","https://registry.docker-cn.com","https://docker.mirrors.ustc.edu.cn","https://dockerhub.azk8s.cn","http://hub-mirror.c.163.com","http://qtid6917.mirror.aliyuncs.com", "https://rncxm540.mirror.aliyuncs.com"],  "exec-opts": ["native.cgroupdriver=systemd"]&#125; #修改 docker 文件驱动为 systemd，默认为 cgroupfs，kubelet 默认使用 systemd，两者必须一致才可以。systemctl daemon-reload && systemctl restart docker && systemctl enable dockersystemctl status docker<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="四、安装初始化-k8s-需要的软件包（所有节点）"><a href="#四、安装初始化-k8s-需要的软件包（所有节点）" class="headerlink" title="四、安装初始化 k8s 需要的软件包（所有节点）"></a>四、安装初始化 k8s 需要的软件包（所有节点）</h1><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext"># 所有节点yum install -y kubelet-1.20.6 kubeadm-1.20.6 kubectl-1.20.6 systemctl enable kubelet && systemctl start kubelet systemctl status kubelet #上面可以看到 kubelet 状态不是 running 状态，这个是正常的，不用管，等 k8s 组件起来这个 注：每个软件包的作用 Kubeadm: kubeadm 是一个工具，用来初始化 k8s 集群的 kubelet: 安装在集群所有节点上，用于启动 Pod 的 kubectl: 通过 kubectl 可以部署和管理应用，查看各种资源，创建、删除和更新各种组件<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="五、-keepalive-nginx-实现-k8s-apiserver-节点高可用"><a href="#五、-keepalive-nginx-实现-k8s-apiserver-节点高可用" class="headerlink" title="五、 keepalive+nginx 实现 k8s apiserver 节点高可用"></a>五、 keepalive+nginx 实现 k8s apiserver 节点高可用</h1><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">#配置 epel 源 把 epel.repo 上传到 master1 的/etc/yum.repos.d 目录下，这样才能安装 keepalived 和nginx #把 epel.repo 拷贝到远程主机 master2 和 node1 上 scp /etc/yum.repos.d/epel.repo master2:/etc/yum.repos.d/ scp /etc/yum.repos.d/epel.repo node1:/etc/yum.repos.d/  1、安装 nginx 主备： 在 master1 和 master2 上做 nginx 主备安装  [root@master1 ~]# yum install nginx keepalived nginx-mod-stream -y [root@master2 ~]# yum install nginx keepalived nginx-mod-stream -y 2、修改 nginx 配置文件。主备一样，上传nginx.conf 到master1 修改本机ip[root@master1 yum.repos.d]# vim /etc/nginx/nginx.conf user nginx;worker_processes auto;error_log /var/log/nginx/error.log;pid /run/nginx.pid;include /usr/share/nginx/modules/*.conf;events &#123;    worker_connections 1024;&#125;# 四层负载均衡，为两台Master apiserver组件提供负载均衡stream &#123;    log_format  main  &#x27;$remote_addr $upstream_addr - [$time_local] $status $upstream_bytes_sent&#x27;;    access_log  /var/log/nginx/k8s-access.log  main;    upstream k8s-apiserver &#123;       server 192.168.162.180:6443;   # Master1 修改本机ip       server 192.168.162.181:6443;   # Master2 修改本机ip    &#125;        server &#123;       listen 16443; # 由于nginx与master节点复用，这个监听端口不能是6443，否则会冲突       proxy_pass k8s-apiserver;    &#125;&#125;http &#123;    log_format  main  &#x27;$remote_addr - $remote_user [$time_local] "$request" &#x27;                      &#x27;$status $body_bytes_sent "$http_referer" &#x27;                      &#x27;"$http_user_agent" "$http_x_forwarded_for"&#x27;;    access_log  /var/log/nginx/access.log  main;    sendfile            on;    tcp_nopush          on;    tcp_nodelay         on;    keepalive_timeout   65;    types_hash_max_size 2048;    include             /etc/nginx/mime.types;    default_type        application/octet-stream;    server &#123;        listen       80 default_server;        server_name  _;        location / &#123;        &#125;    &#125;&#125;# 拷贝配置文件到master2节点[root@master1 yum.repos.d]# scp -r  /etc/nginx/nginx.conf  master2:/etc/nginx/nginx.conf 3、keepalive 配置 # 主 keepalived [root@master1 ~]# vim /etc/keepalived/keepalived.conf global_defs &#123;    notification_email &#123;      acassen@firewall.loc      failover@firewall.loc      sysadmin@firewall.loc    &#125;    notification_email_from Alexandre.Cassen@firewall.loc     smtp_server 127.0.0.1    smtp_connect_timeout 30    router_id NGINX_MASTER&#125; vrrp_script check_nginx &#123;    script "/etc/keepalived/check_nginx.sh"&#125;vrrp_instance VI_1 &#123;     state MASTER     interface ens32  # 修改为实际网卡名    virtual_router_id 51 # VRRP 路由 ID实例，每个实例是唯一的     priority 100    # 优先级，备服务器设置 90     advert_int 1    # 指定VRRP 心跳包通告间隔时间，默认1秒     authentication &#123;         auth_type PASS              auth_pass 1111     &#125;      # 虚拟IP    virtual_ipaddress &#123;         192.168.162.199/24    &#125;     track_script &#123;        check_nginx    &#125; &#125; #vrrp_script：指定检查 nginx 工作状态脚本（根据 nginx 状态判断是否故障转移） #virtual_ipaddress：虚拟 IP（VIP）  [root@master1 ~]# vim /etc/keepalived/check_nginx.sh #!/bin/bash#1、判断Nginx是否存活counter=`ps -C nginx --no-header | wc -l`if [ $counter -eq 0 ]; then    #2、如果不存活则尝试启动Nginx    service nginx start    sleep 2    #3、等待2秒后再次获取一次Nginx状态    counter=`ps -C nginx --no-header | wc -l`    #4、再次进行判断，如Nginx还不存活则停止Keepalived，让地址进行漂移    if [ $counter -eq 0 ]; then        service  keepalived stop    fifi[root@master1 ~]# chmod +x /etc/keepalived/check_nginx.sh    # 备 keepalive [root@master2 ~]# vim /etc/keepalived/keepalived.conf global_defs &#123;    notification_email &#123;      acassen@firewall.loc      failover@firewall.loc      sysadmin@firewall.loc    &#125;    notification_email_from Alexandre.Cassen@firewall.loc     smtp_server 127.0.0.1    smtp_connect_timeout 30    router_id NGINX_MASTER&#125; vrrp_script check_nginx &#123;    script "/etc/keepalived/check_nginx.sh"&#125;vrrp_instance VI_1 &#123;     state MASTER     interface ens32  # 修改为实际网卡名    virtual_router_id 51 # VRRP 路由 ID实例，每个实例是唯一的     priority 90    # 优先级，备服务器设置 90     advert_int 1    # 指定VRRP 心跳包通告间隔时间，默认1秒     authentication &#123;         auth_type PASS              auth_pass 1111     &#125;      # 虚拟IP    virtual_ipaddress &#123;         192.168.162.199/24    &#125;     track_script &#123;        check_nginx    &#125; &#125;  [root@master2 ~]# vim /etc/keepalived/check_nginx.sh #!/bin/bash#1、判断Nginx是否存活counter=`ps -C nginx --no-header | wc -l`if [ $counter -eq 0 ]; then    #2、如果不存活则尝试启动Nginx    service nginx start    sleep 2    #3、等待2秒后再次获取一次Nginx状态    counter=`ps -C nginx --no-header | wc -l`    #4、再次进行判断，如Nginx还不存活则停止Keepalived，让地址进行漂移    if [ $counter -eq 0 ]; then        service  keepalived stop    fifi [root@master2 ~]# chmod +x /etc/keepalived/check_nginx.sh #注：keepalived 根据脚本返回状态码（0 为工作正常，非 0 不正常）判断是否故障转移。  4、启动服务： [root@master1 ~]# systemctl daemon-reload [root@master1 ~]# yum install nginx-mod-stream -y [root@master1 ~]# systemctl start nginx [root@master1 ~]# systemctl start keepalived [root@master1 ~]# systemctl enable nginx keepalived [root@master1]# systemctl status keepalived[root@master2 ~]# systemctl daemon-reload [root@master2 ~]# yum install nginx-mod-stream -y [root@master2 ~]# systemctl start nginx [root@master2 ~]# systemctl start keepalived [root@master2 ~]# systemctl enable nginx keepalived [root@master2]# systemctl status keepalived5、测试 vip 是否绑定成功 [root@master1 ~]# ip addr # 停掉 master1 上的 nginx。Vip 会漂移到 master2 [root@master1 ~]# shutdown now[root@master2]# ip addr#启动 master1 上的 nginx 和 keepalived，vip 又会漂移回来 [root@master1 ~]# systemctl daemon-reload [root@master1 ~]# systemctl start nginx [root@master1 ~]# systemctl start keepalived [root@master1]# ip addr<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="六、kubeadm-初始化-k8s-集群"><a href="#六、kubeadm-初始化-k8s-集群" class="headerlink" title="六、kubeadm 初始化 k8s 集群"></a>六、kubeadm 初始化 k8s 集群</h1><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">#在 master1 上创建 kubeadm-config.yaml 文件： [root@master1 ~]# cd /root/ [root@master1]# vim kubeadm-config.yaml apiVersion: kubeadm.k8s.io/v1beta2kind: ClusterConfigurationkubernetesVersion: v1.20.6controlPlaneEndpoint: 192.168.162.199:16443  # vipimageRepository: registry.aliyuncs.com/google_containersapiServer: certSANs: - 192.168.162.180 - 192.168.162.181 - 192.168.162.182 - 192.168.162.199  # vipnetworking:  podSubnet: 10.244.0.0/16  serviceSubnet: 10.10.0.0/16---apiVersion: kubeproxy.config.k8s.io/v1alpha1kind:  KubeProxyConfigurationmode: ipvs #使用 kubeadm 初始化 k8s 集群 #把初始化 k8s 集群需要的离线镜像包上传到 master1、master2、node1机器上，手动解压： [root@master1 ~]# docker load -i k8simage-1-20-6.tar.gz [root@master2 ~]# docker load -i k8simage-1-20-6.tar.gz [root@node1 ~]# docker load -i k8simage-1-20-6.tar.gz[root@master1]# kubeadm init --config kubeadm-config.yaml --ignore-preflight-errors=SystemVerification  注：--image-repository registry.aliyuncs.com/google_containers：手动指定仓库地址为registry.aliyuncs.com/google_containers。kubeadm 默认从 k8s.grc.io 拉取镜像，但是 k8s.gcr.io访问不到，所以需要指定从 registry.aliyuncs.com/google_containers 仓库拉取镜像。 #配置 kubectl 的配置文件 config，相当于对 kubectl 进行授权，这样 kubectl 命令可以使用这个证书对 k8s 集群进行管理 [root@master1 ~]# mkdir -p $HOME/.kube [root@master1 ~]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config [root@master1 ~]# sudo chown $(id -u):$(id -g) $HOME/.kube/config [root@master1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION master1 NotReady control-plane,master 60s v1.20.6# 此时集群状态还是 NotReady 状态，因为没有安装网络插件。 6、扩容 k8s 集群-添加 master 节点#把 master1 节点的证书拷贝到 master2 上 在 master2 创建证书存放目录： [root@master2 ~]# cd /root && mkdir -p /etc/kubernetes/pki/etcd &&mkdir -p ~/.kube/ #把 master1 节点的证书拷贝到 master2 上： [root@master1 ~]# scp /etc/kubernetes/pki/ca.crt master2:/etc/kubernetes/pki/ [root@master1 ~]# scp /etc/kubernetes/pki/ca.key master2:/etc/kubernetes/pki/ [root@master1 ~]# scp /etc/kubernetes/pki/sa.key master2:/etc/kubernetes/pki/ [root@master1 ~]# scp /etc/kubernetes/pki/sa.pub master2:/etc/kubernetes/pki/ [root@master1 ~]# scp /etc/kubernetes/pki/front-proxy-ca.crt master2:/etc/kubernetes/pki/ [root@master1 ~]# scp /etc/kubernetes/pki/front-proxy-ca.key master2:/etc/kubernetes/pki/ [root@master1 ~]# scp /etc/kubernetes/pki/etcd/ca.crt master2:/etc/kubernetes/pki/etcd/ [root@master1 ~]# scp /etc/kubernetes/pki/etcd/ca.key master2:/etc/kubernetes/pki/etcd/  #证书拷贝之后在 master2 上执行如下命令，大家复制自己的，这样就可以把master2 和加入到集群，成为控制节点：  在 master1 上查看加入节点的命令： [root@master1 ~]# kubeadm token create --print-join-command  [root@master2 ~]#kubeadm join 192.168.40.199:16443 --token zwzcks.u4jd8lj56wpckcwv \  --discovery-token-ca-cert-hash sha256:1ba1b274090feecfef58eddc2a6f45590299c1d0624618f1f429b18a064cb728 \  --control-plane --ignore-preflight-errors=SystemVerification  在 master1 上查看集群状况： [root@master1 ~]# kubectl get nodes7、扩容 k8s 集群-添加 node 节点 在 master1 上查看加入节点的命令： [root@master1 ~]# kubeadm token create --print-join-command #显示如下： kubeadm join 192.168.40.199:16443 --token y23a82.hurmcpzedblv34q8 --discoverytoken-ca-cert-hash sha256:1ba1b274090feecfef58eddc2a6f45590299c1d0624618f1f429b18a064cb728 # 把 node1 加入 k8s 集群： [root@node1~]# kubeadm token create --print-join-command kubeadm join 192.168.40.199:16443 --token y23a82.hurmcpzedblv34q8 --discoverytoken-ca-cert-hash sha256:1ba1b274090feecfef58eddc2a6f45590299c1d0624618f1f429b18a064cb728 --ignore-preflight-errors=SystemVerification #在 master1 上查看集群节点状况： [root@master1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION master1 NotReady control-plane,master 53m v1.20.6 master2 NotReady control-plane,master 5m13s v1.20.6 node1 NotReady <none> 59s v1.20.6  #可以看到 node1 的 ROLES 角色为空，<none>就表示这个节点是工作节点。 #可以把 node1 的 ROLES 变成 work，按照如下方法： [root@master1 ~]# kubectl label node node1 node-role.kubernetes.io/worker=worker[root@master1 ~]# kubectl get pods -n kube-system8、安装 kubernetes 网络组件-Calico 上传 calico.yaml 到 master1 上，使用 yaml 文件安装 calico 网络插件 。 [root@master1 ~]# kubectl apply -f calico.yaml  注：在线下载配置文件地址是： https://docs.projectcalico.org/manifests/calico.yaml 。 [root@master1 ~]# kubectl get pod -n kube-system再次查看集群状态。 [root@master1 ~]# kubectl get nodes9、测试在 k8s 创建 pod 是否可以正常访问网络 #把 busybox-1-28.tar.gz 上传到 node1 节点，手动解压 [root@node1 ~]# docker load -i busybox-1-28.tar.gz [root@master1 ~]# kubectl run busybox --image busybox:1.28 --restart=Never --rm -it busybox -- sh/ # ping www.baidu.comPING www.baidu.com (39.156.66.18): 56 data bytes64 bytes from 39.156.66.18: seq=0 ttl=127 time=39.3 ms#通过上面可以看到能访问网络，说明 calico 网络插件已经被正常安装了 10、测试 k8s 集群中部署 tomcat 服务 #把 tomcat.tar.gz 上传到 node1，手动解压[root@node1 ~]# docker load -i tomcat.tar.gz[root@master1 ~]# kubectl apply -f tomcat.yaml[root@master1 ~]# kubectl get pods NAME READY STATUS RESTARTS AGE demo-pod 1/1 Running 0 10s [root@master1 ~]# kubectl apply -f tomcat-service.yaml[root@master1 ~]# kubectl get svc11、测试 coredns 是否正常 [root@master1 ~]# kubectl run busybox --image busybox:1.28 --restart=Never --rm -it busybox -- sh If you don&#x27;t see a command prompt, try pressing enter. / # nslookup kubernetes.default.svc.cluster.local Server: 10.10.0.10 Address 1: 10.10.0.10 kube-dns.kube-system.svc.cluster.local  Name: kubernetes.default.svc.cluster.local Address 1: 10.10.0.1 kubernetes.default.svc.cluster.local / # nslookup tomcat.default.svc.cluster.local Server: 10.10.0.10 Address 1: 10.10.0.10 kube-dns.kube-system.svc.cluster.local  Name: tomcat.default.svc.cluster.local Address 1: 10.10.13.88 tomcat.default.svc.cluster.local 10.10.13.88 就是我们 coreDNS 的 clusterIP，说明 coreDNS 配置好了。解析内部 Service 的名称，是通过 coreDNS 去解析的。  10.10.0.10 是创建的 tomcat 的 service ip#注意：busybox 要用指定的 1.28 版本，不能用最新版本，最新版本，nslookup 会解析不到 dns 和 ip<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>修改监听端口</p><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext"># 修改kube-scheduler 监听端口vim /etc/kubernetes/manifests/kube-scheduler.yaml修改如下内容：把--bind-address=127.0.0.1 变成--bind-address=192.168.40.180把 httpGet:字段下的 hosts 由 127.0.0.1 变成 192.168.40.180把—port=0 删除#注意：192.168.40.180 是 k8s 的控制节点 master1 的 ip# 修改kube-controller-manager监听端口vim /etc/kubernetes/manifests/kube-controller-manager.yaml把--bind-address=127.0.0.1 变成--bind-address=192.168.40.130把 httpGet:字段下的 hosts 由 127.0.0.1 变成 192.168.40.180把—port=0 删除修改之后在 k8s 各个节点执行systemctl restart kubeletkubectl get cs 显示如下:NAME STATUS MESSAGE ERRORcontroller-manager Healthy ok scheduler Healthy ok etcd-0 Healthy &#123;"health":"true"&#125;ss -antulp | grep :10251ss -antulp | grep :10252# 修改 kube-proxy监听端口kubectl edit configmap kube-proxy -n kube-system把 metricsBindAddress 这段修改成 metricsBindAddress: 0.0.0.0:10249然后重新启动 kube-proxy 这个 pod[root@master1]# kubectl get pods -n kube-system | grep kube-proxy |awk &#x27;&#123;print $1&#125;&#x27; | xargs kubectl delete pods -n kube-system[root@master1]# ss -antulp |grep :10249可显示如下 tcp LISTEN 0 128 [::]:10249 [::]:* 点击 status->targets，可看到如下<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="七、扩展"><a href="#七、扩展" class="headerlink" title="七、扩展"></a>七、扩展</h1><h2 id="1、kubeadm初始化k8s-延长证书过期时间"><a href="#1、kubeadm初始化k8s-延长证书过期时间" class="headerlink" title="1、kubeadm初始化k8s-延长证书过期时间"></a>1、kubeadm初始化k8s-延长证书过期时间</h2><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">查看证书有效时间：[root@master1 ~]# openssl x509 -in /etc/kubernetes/pki/ca.crt -noout -text  |grep Not            Not Before: May  1 04:22:54 2021 GMT            Not After : Apr 29 04:22:54 2031 GMT通过上面可看到ca证书有效期是10年，从2021到2031年[root@master1 ~]# openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -text  |grep Not            Not Before: May  1 04:22:54 2021 GMT            Not After : May  1 04:22:54 2022 GMT通过上面可看到apiserver证书有效期是1年，从2021到2022年：延长证书过期时间1.把update-kubeadm-cert.sh文件上传到master1、master2节点2.在每个节点都执行如下命令1）给update-kubeadm-cert.sh证书授权可执行权限[root@master1 ~]# chmod +x update-kubeadm-cert.sh[root@master2 ~]# chmod +x update-kubeadm-cert.sh2）执行下面命令，修改证书过期时间，把时间延长到10年[root@master1 ~]# ./update-kubeadm-cert.sh all[root@master2 ~]# ./update-kubeadm-cert.sh all3）在master1节点查询Pod是否正常,能查询出数据说明证书签发完成[root@master1 ~]# kubectl  get pods NAME       READY   STATUS    RESTARTS   AGEdemo-pod   1/1     Running   0          15m能够看到pod信息，说明证书签发正常验证证书有效时间是否延长到10年[root@master1 ~]# openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -text  |grep Not            Not Before: May  1 05:43:45 2021 GMT            Not After : Apr 29 05:43:45 2031 GMT通过上面可看到apiserver证书有效期是10年，从2021到2031年：[root@master1 ~]# openssl x509 -in /etc/kubernetes/pki/apiserver-etcd-client.crt  -noout -text  |grep Not            Not Before: May  1 05:43:43 2021 GMT            Not After : Apr 29 05:43:43 2031 GMT通过上面可看到etcd证书有效期是10年，从2020到2030年：Not Before: Apr 22 11:32:24 2021 GMTNot After : Apr 20 11:32:24 2031 GMT[root@master1 ~]# openssl x509 -in /etc/kubernetes/pki/front-proxy-ca.crt  -noout -text  |grep Not            Not Before: May  1 04:22:55 2021 GMT            Not After : Apr 29 04:22:55 2031 GMT通过上面可看到fron-proxy证书有效期是10年，从2021到2031年<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2、kubeadm初始化k8s-删除控制节点-重新把控制节点加入集群步骤"><a href="#2、kubeadm初始化k8s-删除控制节点-重新把控制节点加入集群步骤" class="headerlink" title="2、kubeadm初始化k8s-删除控制节点-重新把控制节点加入集群步骤"></a>2、kubeadm初始化k8s-删除控制节点-重新把控制节点加入集群步骤</h2><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">1、把master2的信息从etcd删除：[root@master1 ~]# tar zxvf etcd-v3.3.4-linux-amd64.tar.gz [root@master1 ~]# cd etcd-v3.3.4-linux-amd64[root@master1 etcd-v3.3.4-linux-amd64]# cp etcdctl /usr/local/sbin/[root@master1 ~]# ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert  /etc/kubernetes/pki/etcd/server.crt --key  /etc/kubernetes/pki/etcd/server.key member list显示如下：1203cdd3ad75e761, started, master1, https://192.168.162.180:2380, https://192.168.162.180:2379b71a3456ad801f24, started, master2, https://192.168.162.181:2380, https://192.168.162.181:23792、找到master2对应的hash值是：b71a3456ad801f243、我们下一步就是根据hash删除etcd信息，执行如下命令[root@master1 ~]# ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key member remove b71a3456ad801f244、查看加入集群命令：[root@master1 ~]kubeadm token create --print-join-commandkubeadm join 192.168.162.199:16443 --token fvby4y.o2m8zhb7j9unpdxt     --discovery-token-ca-cert-hash sha256:1ba1b274090feecfef58eddc2a6f45590299c1d0624618f1f429b18a064cb7285、把master2从k8s集群删除，重新加入到k8s步骤[root@master1 ~]#  kubectl delete nodes master2[root@master2 ~]# kubeadm reset6、把master1上的证书还是按照文档全都拷贝到master2机器上[root@master1 ~]# scp /etc/kubernetes/pki/ca.crt master2:/etc/kubernetes/pki/[root@master1 ~]# scp /etc/kubernetes/pki/ca.key master2:/etc/kubernetes/pki/[root@master1 ~]# scp /etc/kubernetes/pki/sa.key master2:/etc/kubernetes/pki/[root@master1 ~]# scp /etc/kubernetes/pki/sa.pub master2:/etc/kubernetes/pki/[root@master1 ~]# scp /etc/kubernetes/pki/front-proxy-ca.crt master2:/etc/kubernetes/pki/[root@master1 ~]# scp /etc/kubernetes/pki/front-proxy-ca.key master2:/etc/kubernetes/pki/[root@master2 ~]# mkdir /etc/kubernetes/pki/etcd/[root@master1 ~]# scp /etc/kubernetes/pki/etcd/ca.crt master2:/etc/kubernetes/pki/etcd/[root@master1 ~]# scp /etc/kubernetes/pki/etcd/ca.key master2:/etc/kubernetes/pki/etcd/7、在master2执行如下命令，把节点加入k8s集群，充当控制节点：[root@master2 ~]# kubeadm join 192.168.162.199:16443 --token fvby4y.o2m8zhb7j9unpdxt     --discovery-token-ca-cert-hash sha256:1ba1b274090feecfef58eddc2a6f45590299c1d0624618f1f429b18a064cb728 --control-plane8、查看集群是否加入成功：[root@master1 ~]# kubectl get nodesNAME              STATUS   ROLES                  AGE    VERSIONmaster1   Ready    control-plane,master   102m   v1.20.6master2   Ready    control-plane,master   50s    v1.20.6node1     Ready    <none>                       13m    v1.20.6<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3、k8s卸载"><a href="#3、k8s卸载" class="headerlink" title="3、k8s卸载"></a>3、k8s卸载</h2><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">1、只有master节点执行# 必须删除，否则会出现https://www.cnblogs.com/lfl17718347843/p/14122407.html中的问题rm -rf $HOME/.kube2、master和worker节点都要执行# 停掉kubeletsystemctl stop kubelet.service# 重新初始化节点配置，输入字母y回车kubeadm reset# 卸载管理组件yum erase -y kubelet kubectl kubeadm kubernetes-cni# 重启dockersystemctl restart docker<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4、kubernetes强化tab（安装之后会tab可以补全命令及参数）"><a href="#4、kubernetes强化tab（安装之后会tab可以补全命令及参数）" class="headerlink" title="4、kubernetes强化tab（安装之后会tab可以补全命令及参数）"></a>4、kubernetes强化tab（安装之后会tab可以补全命令及参数）</h2><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">echo &#x27;source  <(kubectl  completion  bash)&#x27; >> ~/.bashrc && bash#配置kubectl子命令补全（未试）[root@master1 work]# yum install -y bash-completion[root@master1 work]# source /usr/share/bash-completion/bash_completion[root@master1 work]# source <(kubectl completion bash)[root@master1 work]# kubectl completion bash > ~/.kube/completion.bash.inc[root@master1 work]# source &#x27;/root/.kube/completion.bash.inc&#x27;[root@master1 work]# source $HOME/.bash_profile<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="八、知识点"><a href="#八、知识点" class="headerlink" title="八、知识点"></a>八、知识点</h1><h2 id="问题-：为什么要关闭-swap-交换分区？"><a href="#问题-：为什么要关闭-swap-交换分区？" class="headerlink" title="问题 ：为什么要关闭 swap 交换分区？"></a>问题 ：为什么要关闭 swap 交换分区？</h2><p>Swap 是交换分区，如果机器内存不够，会使用 swap 分区，但是 swap 分区的性能较低，k8s 设计的时候为了能提升性能，默认是不允许使用交换分区的。Kubeadm 初始化的时候会检测 swap 是否关闭，如果没关闭，那就初始化失败。如果不想要关闭交换分区，安装 k8s 的时候可以指定–ignorepreflight-errors&#x3D;Swap 来解决</p><h2 id="问题-：sysctl-是做什么的？"><a href="#问题-：sysctl-是做什么的？" class="headerlink" title="问题 ：sysctl 是做什么的？"></a>问题 ：sysctl 是做什么的？</h2><p>在运行时配置内核参数<br> -p 从指定的文件加载系统参数，如不指定即从&#x2F;etc&#x2F;sysctl.conf 中加载</p><p>问题 2：为什么要执行 modprobe br_netfilter？<br>修改&#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf 文件，增加如下三行参数：<br>net.bridge.bridge-nf-call-ip6tables &#x3D; 1<br>net.bridge.bridge-nf-call-iptables &#x3D; 1<br>net.ipv4.ip_forward &#x3D; 1<br>sysctl -p &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf 出现报错：<br>sysctl: cannot stat &#x2F;proc&#x2F;sys&#x2F;net&#x2F;bridge&#x2F;bridge-nf-call-ip6tables: No such file or<br>directory<br>sysctl: cannot stat &#x2F;proc&#x2F;sys&#x2F;net&#x2F;bridge&#x2F;bridge-nf-call-iptables: No such file or<br>directory<br>解决方法：<br>modprobe br_netfilter</p><h2 id="问题-：为什么开启-net-bridge-bridge-nf-call-iptables-内核参数？"><a href="#问题-：为什么开启-net-bridge-bridge-nf-call-iptables-内核参数？" class="headerlink" title="问题 ：为什么开启 net.bridge.bridge-nf-call-iptables 内核参数？"></a>问题 ：为什么开启 net.bridge.bridge-nf-call-iptables 内核参数？</h2><p>在 centos 下安装 docker，执行 docker info 出现如下警告：<br>WARNING: bridge-nf-call-iptables is disabled<br>WARNING: bridge-nf-call-ip6tables is disabled<br>解决办法：<br>vim &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf<br>net.bridge.bridge-nf-call-ip6tables &#x3D; 1<br>net.bridge.bridge-nf-call-iptables &#x3D; 1<br>问题 4：为什么要开启 net.ipv4.ip_forward &#x3D; 1 参数？<br>kubeadm 初始化 k8s 如果报错：<br>就表示没有开启 ip_forward，需要开启。<br>net.ipv4.ip_forward 是数据包转发：<br>出于安全考虑，Linux 系统默认是禁止数据包转发的。所谓转发即当主机拥有多于一块的网卡时，其中一块收到数据包，根据数据包的目的 ip 地址将数据包发往本机另一块网卡，该网卡根据路由表继续发送数据包。这通常是路由器所要实现的功能。<br>要让 Linux 系统具有路由转发功能，需要配置一个 Linux 的内核参数 net.ipv4.ip_forward。这个参数指定了 Linux 系统当前对路由转发功能的支持情况；其值为 0 时表示禁止进行 IP 转发；如果是 1,则说明 IP 转发功能已经打开</p><h2 id="问题-：ipvs-是什么？"><a href="#问题-：ipvs-是什么？" class="headerlink" title="问题 ：ipvs 是什么？"></a>问题 ：ipvs 是什么？</h2><p>​ipvs (IP Virtual Server) 实现了传输层负载均衡，也就是我们常说的 4 层 LAN 交换，作为 Linux内核的一部分。ipvs 运行在主机上，在真实服务器集群前充当负载均衡器。ipvs 可以将基于 TCP 和 UDP的服务请求转发到真实服务器上，并使真实服务器的服务在单个 IP 地址上显示为虚拟服务。 </p><h2 id="问题-：ipvs-和-iptable-对比分析"><a href="#问题-：ipvs-和-iptable-对比分析" class="headerlink" title="问题 ：ipvs 和 iptable 对比分析"></a>问题 ：ipvs 和 iptable 对比分析</h2><p>kube-proxy 支持 iptables 和 ipvs 两种模式， 在 kubernetes v1.8 中引入了 ipvs 模式，在 v1.9 中处于 beta 阶段，在 v1.11 中已经正式可用了。iptables 模式在 v1.1 中就添加支持了，从 v1.2 版本开始 iptables 就是 kube-proxy 默认的操作模式，ipvs 和 iptables 都是基于 netfilter的，但是 ipvs 采用的是 hash 表，因此当 service 数量达到一定规模时，hash 查表的速度优势就会显现出来，从而提高 service 的服务性能。那么 ipvs 模式和 iptables 模式之间有哪些差异呢？ </p><p>1、ipvs 为大型集群提供了更好的可扩展性和性能 </p><p>2、ipvs 支持比 iptables 更复杂的复制均衡算法（最小负载、最少连接、加权等等） </p><p>3、ipvs 支持服务器健康检查和连接重试等功能</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一、kubeadm-安装多-master-节点的-k8s-集群-1-20-以上稳定版本&quot;&gt;&lt;a href=&quot;#一、kubeadm-安装多-master-节点的-k8s-集群-1-20-以上稳定版本&quot; class=&quot;headerlink&quot; title=&quot;一、kube</summary>
      
    
    
    
    <category term="kubenetets" scheme="http://kjiang.com/categories/kubenetets/"/>
    
    
    <category term="kubenetets" scheme="http://kjiang.com/tags/kubenetets/"/>
    
  </entry>
  
  <entry>
    <title>一、Dockers核心技术精讲</title>
    <link href="http://kjiang.com/posts/4181736639.html"/>
    <id>http://kjiang.com/posts/4181736639.html</id>
    <published>2025-03-11T06:39:04.000Z</published>
    <updated>2025-03-24T14:28:06.825Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、Dockers核心技术精讲"><a href="#一、Dockers核心技术精讲" class="headerlink" title="一、Dockers核心技术精讲"></a>一、Dockers核心技术精讲</h1><p>虚拟化起源史：</p><p>​<strong>第一阶段：Linux服务器</strong></p><p>​服务器就是在机房里放的有完整操作系统的机器，Linux 是一个操作系统，linux 服务器就和 Windows 的服务器是一样的，都是在网络环境或分布式处理环境中，为用户提供服务，可分为访问服务器、文件服务器、数据库服务器、通信服务器和应用服务器等。常见的 Linux 服务器的系统主要包括 debian 系（debian、ubuntu）、redhat 系(redhat、centos)，suse、rockylinux、麒麟、欧拉等，同一类发行版用法相似。</p><p>​优点：</p><p>​1）提供稳定性，因为基于 Linux 的服务器不容易崩溃。在遇到碰撞的情况下，整个系统都不受影响。</p><p>​2）Linux 非常适合阻止或防止可疑恶意软件进入并影响整个系统的性能。</p><p>​3）拥有耐用性，因为它可以长时间运行，一年不关机也不会影响性能。</p><p>​4）大多数 Linux 发行版是免费下载的，可以安装在自己个人电脑上</p><p>​<strong>第二阶段：虚拟机</strong></p><p>​虚拟机（VM）指通过软件模拟的具有完整硬件系统功能的、运行在一个完全隔离环境中的完整计算机系统。简单讲是可以实现在一台物理计算机上模拟多台计算机运行任务。常见虚拟机：Vmware Workstation、VirtualBox、KVM、Hyper-V、Xen 等</p><p>​优点：</p><p>​1、一台主机，通过安装不同系统环境的虚拟机、与运行单独的实体服务器相比是经济的。</p><p>​2、虚拟机与主机操作系统隔离，<strong>虚拟机上的操作不影响主机</strong></p><p>​<strong>第三阶段：容器化</strong></p><p>​容器是一种沙盒技术，主要目的是为了将应用运行在其中，与外界隔离；及方便这个沙盒可以被转移到其它宿主机器。通俗点的理解就是一个装应用软件的箱子，箱子里面有软件运行所需的依赖库和配置。开发人员可以把这个箱子搬到任何机器上，且不影响里面软件的运行。</p><p>​优点：</p><p>​1.环境的强一致性。docker 镜像提供了除内核外完整的运行时环境，保证其他使用人员，能完整复现应用运行时环境。不会出现“这应用在 xxx 环境没问题”这种情况。</p><p>​2.迁移方便，持续交付和部署。一次打包，到处运行。</p><p><strong>所以，欢迎进入容器化时代，目前主流容器，分别是：Docker，k8s(kubernetes)</strong></p><p><strong>注：</strong></p><p><strong>k8s1.24版本和1.24之后的版本使用：containerd</strong></p><p><strong>k8s1.24之前：docker容器运行时</strong></p><h1 id="二、Docker的概述"><a href="#二、Docker的概述" class="headerlink" title="二、Docker的概述"></a>二、Docker的概述</h1><h2 id="1、背景"><a href="#1、背景" class="headerlink" title="1、背景"></a>1、背景</h2><p>​Docker 的思想来自于集装箱，集装箱解决了什么问题？在一艘大船上，可以把货物规整的摆放起来。并且各种各样的货物被装在集装箱里，集装箱和集装箱之间不会互相影响。那么我就不需要专门运送蔬菜的船和专门运送货物的船了。只要这些货物在集装箱里封装的好好的，那我就可以用一艘大船把他们都运走。 docker 就是类似的理念。云计算就好比大货轮。docker 就是集装箱。 </p><h2 id="2、docker-的优点"><a href="#2、docker-的优点" class="headerlink" title="2、docker 的优点"></a>2、docker 的优点</h2><p>​1）快 </p><p>运行时的性能快，管理操作(启动，停止，开始，重启等等) 都是以秒或毫秒为单位的。 </p><p>​2）敏捷 </p><p>像虚拟机一样敏捷，而且会更便宜，在 bare metal(裸机)上布署像点个按钮一样简单。 </p><p>​3）灵活 </p><p>将应用和系统“容器化”，不添加额外的操作系统 </p><p>​4）轻量 </p><p>在一台服务器上可以布署 100~1000 个 Containers 容器。 </p><p>​5）便宜 </p><p>开源的，免费的，低成本的。 </p><p><strong>docker-ce：社区版</strong> </p><p><strong>docker-ee: 商业版</strong> </p><h2 id="3、docker-缺点"><a href="#3、docker-缺点" class="headerlink" title="3、docker 缺点"></a><strong>3、docker 缺点</strong></h2><p><strong>所有容器共用 linux kernel 资源，资源能否实现最大限度利用，所以在安全上也会存在漏洞。</strong> </p><h2 id="4、docker安装"><a href="#4、docker安装" class="headerlink" title="4、docker安装"></a>4、docker安装</h2><h3 id="4-1-安装基础环境"><a href="#4-1-安装基础环境" class="headerlink" title="4.1 安装基础环境"></a>4.1 安装基础环境</h3><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext"># 配置主机名hostnamectl set-hostname jiang && bash # 关闭防火墙systemctl stop firewalld && systemctl disable firewalld# 关闭 iptables 防火墙yum install iptables-services -y #安装 iptablesservice iptables stop && systemctl disable iptables# 清空防火墙规则iptables -F#关闭 selinuxsetenforce 0sed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27; /etc/selinux/config# 注意：修改 selinux 配置文件之后，重启机器，selinux 才能永久生效getenforce# 显示 Disabled 表示 selinux 关闭成功#配置时间同步 yum install -y ntp ntpdate ntpdate cn.pool.ntp.org #编写计划任务 crontab -e * */1 * * * /usr/sbin/ntpdate cn.pool.ntp.org # 重启 crond 服务使配置生效： systemctl restart crond# 安装基础软件包yum install -y wget net-tools nfs-utils lrzsz gcc gcc-c++ make cmake libxml2-devel openssl-devel curl curl-devel unzip sudo ntp libaio-devel wget vim ncurses-devel autoconf automake zlib-devel python-devel epel-release openssh-server socat ipvsadm conntrack<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-2-安装Docker-ce"><a href="#4-2-安装Docker-ce" class="headerlink" title="4.2 安装Docker-ce"></a>4.2 安装Docker-ce</h3><pre class="line-numbers language-hljs bash"><code class="language-hljs bash"><span class="hljs-comment"># 安装 docker 依赖包 </span>yum install -y yum-utils device-mapper-persistent-data lvm2 <span class="hljs-comment"># 配置 docker-ce 国内 yum 源（阿里云） </span>yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo  <span class="hljs-comment"># 查看docker版本</span>yum list docker-ce --showduplicates | <span class="hljs-built_in">sort</span> -r<span class="hljs-comment"># 安装指定版本</span>yum install -y docker-ce-18.09.9 docker-ce-cli-18.09.9 containerd.io<span class="hljs-comment"># 安装 docker-ce 最新版</span>yum install docker-ce -y   <span class="hljs-comment"># 启动 docker 服务 </span>systemctl start docker && systemctl <span class="hljs-built_in">enable</span> docker systemctl status docker<span class="hljs-comment"># 查看 Docker 版本信息</span>docker version<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-3-开启包转发功能和修改内核参数"><a href="#4-3-开启包转发功能和修改内核参数" class="headerlink" title="4.3 开启包转发功能和修改内核参数"></a>4.3 <strong>开启包转发功能和修改内核参数</strong></h3><p>内核参数修改：br_netfilter 模块用于将桥接流量转发至 iptables 链，br_netfilter 内核参数需要开启转发。</p><p>不开启ipv4转发的话，在容器中暴露端口，访问不到地址。、</p><pre class="line-numbers language-hljs bash"><code class="language-hljs bash">modprobe br_netfilter <span class="hljs-built_in">cat</span> > /etc/sysctl.d/docker.conf << <span class="hljs-string">EOF </span><span class="hljs-string">net.bridge.bridge-nf-call-ip6tables = 1 </span><span class="hljs-string">net.bridge.bridge-nf-call-iptables = 1 </span><span class="hljs-string">net.ipv4.ip_forward = 1 </span><span class="hljs-string">EOF</span>  <span class="hljs-comment"># 使参数生效 </span>sysctl -p /etc/sysctl.d/docker.conf  <span class="hljs-comment"># 重启后模块失效，下面是开机自动加载模块的脚本 </span><span class="hljs-comment"># 在/etc/新建 rc.sysinit 文件 </span><span class="hljs-built_in">cat</span> /etc/rc.sysinit <span class="hljs-comment">#!/bin/bash </span><span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> /etc/sysconfig/modules/*.modules ; <span class="hljs-keyword">do</span> [ -x <span class="hljs-variable">$file</span> ] && <span class="hljs-variable">$file</span> <span class="hljs-keyword">done</span>  <span class="hljs-comment"># 在/etc/sysconfig/modules/目录下新建文件如下 </span><span class="hljs-built_in">cat</span> /etc/sysconfig/modules/br_netfilter.modules modprobe br_netfilter  <span class="hljs-comment"># 输出</span> <span class="hljs-comment"># 增加权限 </span><span class="hljs-built_in">chmod</span> 755 /etc/sysconfig/modules/br_netfilter.modules  <span class="hljs-comment"># 重启机器模块也会自动加载 </span>lsmod |grep br_netfilter <span class="hljs-comment"># 输出</span>br_netfilter 22209 0 bridge 136173 1 br_netfilter<span class="hljs-comment"># 重启docker</span> systemctl restart docker<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-4-开启云加速器"><a href="#4-4-开启云加速器" class="headerlink" title="4.4 开启云加速器"></a>4.4 开启云加速器</h3><p>登录阿里云镜像仓库：<a href="https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors">https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors</a> </p><pre class="line-numbers language-hljs bash"><span class="hljs-built_in">sudo</span> <span class="hljs-built_in"><code class="language-hljs bash"><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">mkdir</span> -p /etc/docker<span class="hljs-built_in">sudo</span> <span class="hljs-built_in">tee</span> /etc/docker/daemon.json <<-<span class="hljs-string">&#x27;EOF&#x27;</span>&#123;  <span class="hljs-string">"registry-mirrors"</span>: [<span class="hljs-string">"https://b07cesj8.mirror.aliyuncs.com"</span>]&#125;EOF<span class="hljs-built_in">sudo</span> systemctl daemon-reload<span class="hljs-built_in">sudo</span> systemctl restart docker<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext"># 其他镜像加速器# docker中国区官方镜像加速：https://registry.docker-cn.com# 网易镜像加速：http://hub-mirror.c.163.com# 中国科技大学镜像加速：https://docker.mirrors.ustc.edu.cn# 腾讯云镜像加速：https://mirror.ccs.tencentyun.com<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-5-实战nginx"><a href="#4-5-实战nginx" class="headerlink" title="4.5 实战nginx"></a>4.5 实战nginx</h3><pre class="line-numbers language-hljs bash"><span class="hljs-comment"><code class="language-hljs bash"><span class="hljs-comment"># 创建容器</span>docker run --name centos -p 80 -itd centosdocker ps | grep centos<span class="hljs-comment"># 进入容器</span>docker <span class="hljs-built_in">exec</span> -it centos /bin/bash <span class="hljs-comment"># 由于centos不在维护，需要修改yum源</span><span class="hljs-built_in">rm</span> -rf /etc/yum.repos.d/* curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-vault-8.5.2111.repo <span class="hljs-comment"># 安装插件</span>yum install wget -y yum install nginx -y<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="5、docker问题："><a href="#5、docker问题：" class="headerlink" title="5、docker问题："></a>5、docker问题：</h2><h3 id="Docker-安装后出现：WARNING-bridge-nf-call-iptables-is-disabled-的解决办法："><a href="#Docker-安装后出现：WARNING-bridge-nf-call-iptables-is-disabled-的解决办法：" class="headerlink" title="Docker 安装后出现：WARNING: bridge-nf-call-iptables is disabled 的解决办法："></a><strong>Docker 安装后出现：WARNING: bridge-nf-call-iptables is disabled 的解决办法：</strong></h3><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext"># 参考4.3章节net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>​将 Linux 系统作为路由或者 VPN 服务就必须要开启 IP 转发功能。当 linux 主机有多个网卡时一个网卡收到的信息是否能够传递给其他的网卡 ，如果设置成 1 的话 可以进行数据包转发，可以实现 VxLAN 等功能。不开启会导致 docker 部署应用无法访问。 </p></blockquote><h1 id="三、containerd-容器概述"><a href="#三、containerd-容器概述" class="headerlink" title="三、containerd 容器概述"></a>三、containerd 容器概述</h1><p><strong>官方文档:</strong></p><p><strong><a href="https://containerd.io/">https://containerd.io</a></strong></p><p>在 2016 年 12 月 14 日，Docker 公司宣布将 containerd 从 Docker 中分离，由开源社区独立发展和运营。Containerd 完全可以单独运行并管理容器，而 Containerd 的主要职责是镜像管理和容器执行。同时，Containerd 提供了 containerd-shim 接口封装层，向下继续对接 runC 项目，使得容器引擎 Docker Daemon 可以独立升级。</p><p>Containerd 可以在宿主机中管理完整的容器生命周期：容器镜像的传输和存储、容器的执行和管理、存储和网络等。总结一下，它主要负责干以下事情：</p><p>​• 管理容器的生命周期(从创建容器到销毁容器)</p><p>​• 拉取&#x2F;推送容器镜像</p><p>​• 存储管理(管理镜像及容器数据的存储)</p><p>​• 调用 runC 运行容器(与 runC 等容器运行时交互)</p><p>​• 管理容器网络接口及网络</p><p><strong>ctr：containerd 的命令行客户端。</strong></p><h2 id="1、Containerd-和-Docker-之间的关系"><a href="#1、Containerd-和-Docker-之间的关系" class="headerlink" title="1、Containerd 和 Docker 之间的关系"></a><strong>1、Containerd 和 Docker 之间的关系</strong></h2><p>​Docker 包含 Containerd，Containerd 专注于运行时的容器管理，而 Docker 除了容器管理之外，还可以完成镜像构建之类的功能。</p><p>​Containerd 提供的 API 偏底层，不是给普通用户直接用的，容器编排的开发者才需要Containerd。</p><h2 id="2、Containerd-在容器生态中扮演的角色"><a href="#2、Containerd-在容器生态中扮演的角色" class="headerlink" title="2、Containerd 在容器生态中扮演的角色"></a><strong>2、Containerd 在容器生态中扮演的角色</strong></h2><p>Containerd 并不是直接面向最终用户的，而是主要用于集成到更上层的系统里，比如 Kubernetes等容器编排系统。</p><p>Containerd 以 daemon 的形式运行在系统上，通过 unix domain socket 暴露底层的 grpc API，上层系统可以通过这些 API 管理机器上的容器。</p><h2 id="2-1-K8S-为什么要放弃使用-Docker-作为容器运行时，而使用-containerd-呢？"><a href="#2-1-K8S-为什么要放弃使用-Docker-作为容器运行时，而使用-containerd-呢？" class="headerlink" title="2-1 K8S 为什么要放弃使用 Docker 作为容器运行时，而使用 containerd 呢？"></a>2-1 K8S 为什么要放弃使用 Docker 作为容器运行时，而使用 containerd 呢？</h2><p><strong>说白了，就是containerd更接近底层，跟docker比，占用资源更少，而且docker底层也是用的containerd</strong></p><p>​Docker，Kubernetes 等工具来运行一个容器时会调用容器运行时（CRI），比如 containerd，CRI-O，通过容器运行时来完成容器的创建、运行、销毁等实际工作，Docker 使用的是 containerd 作为其运行时；<strong>Kubernetes 支持 docker（在 k8s1.24 版本之前用，1.24 开始废弃了</strong>）、containerd，</p><p>​CRI-O 等多种容器运行时，这些容器运行时都遵循了 OCI 规范，并通过 runc 来实现与操作系统内核交互来完成容器的创建和运行</p><p><strong>CRI</strong></p><p>​CRI 是一个插件接口，它使 kubelet 能够使用各种容器运行时，你需要在集群中的每个节点上都有一个</p><p>可以正常工作的容器运行时， 这样 kubelet 能启动 Pod 及其容器。容器运行时接口（CRI）是</p><p>kubelet 和容器运行时之间通信的主要协议。</p><p><strong>OCI</strong>：</p><p>​OCI， Open Container Initiative ，是一个轻量级，开放的治理结构（项目），在 Linux 基金会的支</p><p>持下成立，致力于围绕容器格式和运行时创建开放的行业标准。 OCI 项目由 Docker，CoreOS（后来</p><p>被 Red Hat 收购了，相应的席位被 Red Hat 继承）和容器行业中的其他领导者在 2015 年 6 月的时</p><p>候启动。</p><p>如果你使用 Docker 作为 K8S 容器运行时的话，kubelet 需要先要通过 dockershim 去调用 Docker，</p><p>再通过 Docker 去调用 containerd。</p><p>如果你使用 containerd 作为 K8S 容器运行时的话， kubelet 可以直接调用 containerd。</p><p>使用 containerd 不仅性能提高了（调用链变短了），而且资源占用也会变小（Docker 不是一个纯粹的</p><p>容器运行时，具有大量其他功能）。</p><p><strong>调用链</strong></p><p>Docker 作为 k8s 容器运行时，调用关系如下：</p><p>​kubelet –&gt; docker shim （在 kubelet 进程中） –&gt; dockerd –&gt; containerd</p><p>Containerd 作为 k8s 容器运行时，调用关系如下：</p><p>​kubelet –&gt; cri plugin（在 containerd 进程中） –&gt; containerd</p><h2 id="3、containerd-安装和配置"><a href="#3、containerd-安装和配置" class="headerlink" title="3、containerd 安装和配置"></a>3、containerd 安装和配置</h2><p>注：安装 docker 会自动把 containerd 安装出来，也可以通过如下命令直接安装 containerd。</p><pre class="line-numbers language-hljs bash"><span class="hljs-comment"><code class="language-hljs bash"><span class="hljs-comment"># 需要配置 docker-ce.repo 这个 yum 源：</span>yum install yum-utils -yyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoyum install containerd -ysystemctl <span class="hljs-built_in">enable</span> containerdsystemctl start containerd<span class="hljs-comment"># 初始化 Containerd 配置：</span>containerd config default > /etc/containerd/config.tomlsystemctl <span class="hljs-built_in">enable</span> containerdsystemctl start containerd<span class="hljs-comment"># 替换 containerd 默认的 sand_box 镜像，编辑/etc/containerd/config.toml 文件：</span>sandbox_image = <span class="hljs-string">"k8s.gcr.io/pause:3.2"</span> 替换成 registry.cnhangzhou.aliyuncs.com/google_containers/pause-amd64:3.2<span class="hljs-comment"># 应用配置并重新运行 containerd 服务</span>systemctl daemon-reloadsystemctl restart containerdsystemctl status containerd<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4、containerd使用"><a href="#4、containerd使用" class="headerlink" title="4、containerd使用"></a>4、containerd使用</h2><p>​<strong>containerd和docker不同，他通过名称空间进行隔离资源，docker和containerd的镜像可以通用，因为docker底层用的containerd</strong></p><pre class="line-numbers language-hljs bash"><span class="hljs-comment"><code class="language-hljs bash"><span class="hljs-comment"># 查看 containerd 命名空间</span>ctr namespace <span class="hljs-built_in">ls</span><span class="hljs-comment"># ctr 有命名空间 namespace 来指定类似于工作空间的隔离区域。使用方法 ctr -n default images ls </span><span class="hljs-comment"># 来查看 default 命名空间的镜像，不加 -n 参数，默认也是使用 default 的命名空间。</span><span class="hljs-comment">#查看默认名称空间镜像有哪些</span>ctr image <span class="hljs-built_in">ls</span><span class="hljs-comment">#查看 k8s 命名空间下的镜像</span>ctr -n=k8s.io images <span class="hljs-built_in">ls</span><span class="hljs-comment"># 拉取 busybox 镜像</span>ctr image pull docker.io/library/busybox:latest<span class="hljs-comment"># 注：必须全路径，从 dockerhub 上下载默认 busybox 镜像。</span><span class="hljs-comment"># 删除镜像</span>ctr images <span class="hljs-built_in">rm</span> <镜像名称><span class="hljs-comment"># 压缩镜像</span>ctr images pull docker.io/library/mysql:latestctr images <span class="hljs-built_in">export</span> mysql.tar.gz docker.io/library/mysql:latest<span class="hljs-comment"># 导入镜像</span>ctr images import mysql.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="5、docker-的镜像和-containerd-镜像通用吗？"><a href="#5、docker-的镜像和-containerd-镜像通用吗？" class="headerlink" title="5、docker 的镜像和 containerd 镜像通用吗？"></a>5、docker 的镜像和 containerd 镜像通用吗？</h2><p>通用的，docker save -o 生成的镜像文件，可以基于 ctr images import 导出来</p><h1 id="四、Dockerfile语法详解"><a href="#四、Dockerfile语法详解" class="headerlink" title="四、Dockerfile语法详解"></a>四、Dockerfile语法详解</h1><p>Dockerfile 是一个用来构建镜像的文本文件，文本内容包含了一条条构建镜像所需的指令和说明。 </p><pre><code>基于 Dockerfile 构建镜像可以使用 docker build 命令。docker build 命令中使用-f 可以指定具体的</code></pre><p>dockerfile 文件 </p><p>示例：</p><pre class="line-numbers language-hljs dockerfile"><span class="hljs-keyword"><code class="language-hljs dockerfile"><span class="hljs-keyword">FROM</span> centos <span class="hljs-keyword">MAINTAINER</span> jiang<span class="hljs-keyword">RUN</span><span class="language-bash"> <span class="hljs-built_in">rm</span> -rf /etc/yum.repos.d/* </span><span class="hljs-keyword">COPY</span><span class="language-bash"> Centos-vault-8.5.2111.repo /etc/yum.repos.d/ </span><span class="hljs-keyword">RUN</span><span class="language-bash"> yum install wget -y </span><span class="hljs-keyword">ADD</span><span class="language-bash"> jdk-8u45-linux-x64.rpm /usr/local/ </span><span class="hljs-keyword">ADD</span><span class="language-bash"> apache-tomcat-8.0.26.tar.gz /usr/local/ </span><span class="hljs-keyword">RUN</span><span class="language-bash"> <span class="hljs-built_in">cd</span> /usr/local && rpm -ivh jdk-8u45-linux-x64.rpm </span><span class="hljs-keyword">RUN</span><span class="language-bash"> <span class="hljs-built_in">mv</span> /usr/local/apache-tomcat-8.0.26 /usr/local/tomcat8 </span><span class="hljs-keyword">ENTRYPOINT</span><span class="language-bash"> /usr/local/tomcat8/bin/startup.sh && <span class="hljs-built_in">tail</span> -F </span>/usr/local/tomcat8/logs/catalina.out <span class="hljs-keyword">EXPOSE</span> <span class="hljs-number">8080</span><span class="hljs-comment"># docker build -t mycentos .</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>dockerfile 构建过程： </p><ol><li>​从基础镜像运行一个容器 </li><li>​执行一条指令，对容器做出修改 </li><li>​执行类似 docker commit 的操作，提交一个新的镜像层 </li><li>​再基于刚提交的镜像运行一个新的容器 </li><li>​执行 dockerfile 中的下一条指令，直至所有指令执行完毕</li></ol><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">参数简单描述：（1）FROM # 拉取镜像（2）MAINTAINER # 指定镜像的作者信息 （3）RUN # 指定在当前镜像构建过程中要运行的命令 ，格式：RUN echo hello（4）EXPOSE # 暴露端口， 格式：EXPOSE <端口 1> [<端口 2>...] （5）CMD # 用于启动镜像，最后执行，格式：  CMD[“executable”，“param1”，“param2”]（6）ENTERYPOINT # 和CMD类似，最后一个执行，格式：ENTERYPOINT [“executable”,“param1”,“param2”]（7）COPY # 复制文件到容器，COPY [--chown=<user>:<group>] <源路径 1>... <目标路径> （8）ADD # 将文件拷贝到容器，进行解压，单纯复制用copy命令。格式：ADD <src> <dest>（9）VOLUME # 指定数据卷，格式：VOLUME ["<路径 1>", "<路径 2>"...] || VOLUME <路径>  （10）WORKDIR # 指定进入容器后的位置，格式：WORKDIR <工作目录路径>  (11) ENV # 设置环境变量，格式： ENV <key> <value> ||ENV <key>=<value>... （12）USER # 指定进入容器的用户，必须存在，格式：USER <用户名>[:<用户组>] （13）ONBUILD # 如果其他容器用该容器做基础镜像执行。（14）LABEL # 容器打标签，LABEL authors="jiang"  images="jiang"（多个空格隔开）（15）HEALTHCHECK # 容器健康检查，判断容器是否启动，屏蔽检查 HEALTHCHECK NONE，最后一个执行（16）ARG # 环境变量，仅对dockerfile文件生效<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-hljs dockerfile"><code class="language-hljs dockerfile">参数详细说明：（<span class="hljs-number">1</span>）<span class="hljs-keyword">FROM</span> 基础镜像，必须是可以下载下来的，定制的镜像都是基于 <span class="hljs-keyword">FROM</span> 的镜像，这里的 centos 就是定制需要的基础镜像。后续的操作都是基于 centos 镜像。 （<span class="hljs-number">2</span>）<span class="hljs-keyword">MAINTAINER</span> 指定镜像的作者信息  （<span class="hljs-number">3</span>）<span class="hljs-keyword">RUN</span><span class="language-bash">：指定在当前镜像构建过程中要运行的命令 </span>    包含两种模式     <span class="hljs-number">1</span>、<span class="hljs-keyword">Shell</span><span class="language-bash"> </span>    <span class="hljs-keyword">RUN</span><span class="language-bash"> <<span class="hljs-built_in">command</span>> (shell 模式，这个是最常用的，需要记住) </span>    <span class="hljs-keyword">RUN</span><span class="language-bash"> <span class="hljs-built_in">echo</span> hello </span>    <span class="hljs-number">2</span>、exec 模式    <span class="hljs-keyword">RUN</span><span class="language-bash"> [“executable”，“param1”，“param2”](<span class="hljs-built_in">exec</span> 模式) </span>    <span class="hljs-keyword">RUN</span><span class="language-bash"> [“/bin/bash”,”-c”,”<span class="hljs-built_in">echo</span> hello”]</span>    等价于/bin/bash -c echo hello （<span class="hljs-number">4</span>）<span class="hljs-keyword">EXPOSE</span> 指令     仅仅只是声明端口。     作用：     <span class="hljs-number">1</span>、帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射。     <span class="hljs-number">2</span>、在运行时使用随机端口映射时，也就是 docker <span class="hljs-keyword">run</span><span class="language-bash"> -P 时，会自动随机映射 EXPOSE 的端口。 </span>    <span class="hljs-number">3</span>、可以是一个或者多个端口，也可以指定多个 <span class="hljs-keyword">EXPOSE</span>     格式：<span class="hljs-keyword">EXPOSE</span> <端口 <span class="hljs-number">1</span>> [<端口 <span class="hljs-number">2</span>>...]  （<span class="hljs-number">5</span>）<span class="hljs-keyword">CMD</span><span class="language-bash"> </span>    类似于 <span class="hljs-keyword">RUN</span><span class="language-bash"> 指令，用于运行程序，但二者运行的时间点不同: </span>    <span class="hljs-number">1</span>、<span class="hljs-keyword">CMD</span><span class="language-bash"> 在 docker run 时运行。 </span>    <span class="hljs-number">2</span>、<span class="hljs-keyword">RUN</span><span class="language-bash"> 是在 docker build 构建镜像时运行的 </span>    作用：为启动的容器指定默认要运行的程序，程序运行结束，容器也就结束。<span class="hljs-keyword">CMD</span><span class="language-bash"> 指令指定的程序可被</span>    docker <span class="hljs-keyword">run</span><span class="language-bash"> 命令行参数中指定要运行的程序所覆盖。</span>    <span class="hljs-keyword">CMD</span><span class="language-bash">[“executable”，“param1”，“param2”]（<span class="hljs-built_in">exec</span> 模式） </span>    <span class="hljs-keyword">CMD</span><span class="language-bash"> <span class="hljs-built_in">command</span> （shell 模式） </span>    <span class="hljs-keyword">CMD</span><span class="language-bash"> [“param1”,”param2”](作为 ENTRYPOINT 指令的默认参数) </span>（<span class="hljs-number">6</span>）ENTERYPOINT     类似于 <span class="hljs-keyword">CMD</span><span class="language-bash"> 指令，但其不会被 docker run 的命令行参数指定的指令所覆盖，而且这些命令行参数会被</span>    当作参数送给 <span class="hljs-keyword">ENTRYPOINT</span><span class="language-bash"> 指令指定的程序。 </span>    但是, 如果运行 docker <span class="hljs-keyword">run</span><span class="language-bash"> 时使用了 --entrypoint 选项，将覆盖 entrypoint 指令指定的程序。 </span>    优点：在执行 docker <span class="hljs-keyword">run</span><span class="language-bash"> 的时候可以指定 ENTRYPOINT 运行所需的参数。 </span>    注意：如果 Dockerfile 中如果存在多个 <span class="hljs-keyword">ENTRYPOINT</span><span class="language-bash"> 指令，仅最后一个生效。 </span>    格式：     ENTERYPOINT [“executable”,“param1”,“param2”](exec 模式)     ENTERYPOINT command （<span class="hljs-keyword">shell</span><span class="language-bash"> 模式） </span>    可以搭配 <span class="hljs-keyword">CMD</span><span class="language-bash"> 命令使用：一般是变参才会使用 CMD ，这里的 CMD 等于是在给 ENTRYPOINT 传参，以下</span>    示例会提到。     示例：     假设已通过 Dockerfile 构建了 nginx:test 镜像：     <span class="hljs-keyword">FROM</span> nginx     <span class="hljs-keyword">ENTRYPOINT</span><span class="language-bash"> [<span class="hljs-string">"nginx"</span>, <span class="hljs-string">"-c"</span>] <span class="hljs-comment"># 定参 </span></span>    <span class="hljs-keyword">CMD</span><span class="language-bash"> [<span class="hljs-string">"/etc/nginx/nginx.conf"</span>] <span class="hljs-comment"># 变参 </span></span>    <span class="hljs-number">1</span>、不传参运行     $ docker <span class="hljs-keyword">run</span><span class="language-bash"> nginx:<span class="hljs-built_in">test</span> </span>    容器内会默认运行以下命令，启动主进程。     nginx -c /etc/nginx/nginx.conf     <span class="hljs-number">2</span>、传参运行     $ docker <span class="hljs-keyword">run</span><span class="language-bash"> nginx:<span class="hljs-built_in">test</span> -c /etc/nginx/new.conf </span>    容器内会默认运行以下命令，启动主进程(/etc/nginx/new.conf:假设容器内已有此文件)     nginx -c /etc/nginx/new.conf  （<span class="hljs-number">7</span>）<span class="hljs-keyword">COPY</span><span class="language-bash"> </span>    <span class="hljs-keyword">COPY</span><span class="language-bash"><src>..<dest> </span>    <span class="hljs-keyword">COPY</span><span class="language-bash">[“<src>”...“<dest>”] </span>    复制指令，从上下文目录中复制文件或者目录到容器里指定路径。     格式：     <span class="hljs-keyword">COPY</span><span class="language-bash"> [--<span class="hljs-built_in">chown</span>=<user>:<group>] <源路径 1>... <目标路径> </span>    <span class="hljs-keyword">COPY</span><span class="language-bash"> [--<span class="hljs-built_in">chown</span>=<user>:<group>] [<span class="hljs-string">"<源路径 1>"</span>,... <span class="hljs-string">"<目标路径>"</span>]</span>    [--chown=<<span class="hljs-keyword">user</span>>:<group>]：可选参数，用户改变复制到容器内文件的拥有者和属组。    规则。例如：         <span class="hljs-keyword">COPY</span><span class="language-bash"> hom* /mydir/ </span>        <span class="hljs-keyword">COPY</span><span class="language-bash"> hom?.txt /mydir/ </span>        <目标路径>：容器内的指定路径，该路径不用事先建好，路径不存在的话，会自动创建。 （<span class="hljs-number">8</span>）<span class="hljs-keyword">ADD</span><span class="language-bash"> </span>    <span class="hljs-keyword">ADD</span><span class="language-bash"> <src>...<dest> </span>    <span class="hljs-keyword">ADD</span><span class="language-bash"> [“<src>”...“<dest>”] </span>    <span class="hljs-keyword">ADD</span><span class="language-bash"> 指令和 COPY 的使用格式一致（同样需求下，官方推荐使用 COPY）。功能也类似，不同之处如下： </span>    <span class="hljs-keyword">ADD</span><span class="language-bash"> 的优点：在执行 <源文件> 为 tar 压缩文件的话，压缩格式为 gzip, bzip2 以及 xz 的情况下，</span>    会自动复制并解压到 <目标路径>。     <span class="hljs-keyword">ADD</span><span class="language-bash"> 的缺点：在不解压的前提下，无法复制 tar 压缩文件。会令镜像构建缓存失效，从而可能会令镜像</span>    构建变得比较缓慢。具体是否使用，可以根据是否需要自动解压来决定。     <span class="hljs-keyword">ADD</span><span class="language-bash"> vs COPY </span>    <span class="hljs-keyword">ADD</span><span class="language-bash"> 包含类似 tar 的解压功能 </span>    如果单纯复制文件，dockerfile 推荐使用 <span class="hljs-keyword">COPY</span><span class="language-bash"> </span>    例;替换/usr/share/nginx 下的 index.html     cd /root/dockerfile/test1     cat dockerfile     <span class="hljs-keyword">FROM</span> centos     <span class="hljs-keyword">MAINTAINER</span> xianchao     <span class="hljs-keyword">RUN</span><span class="language-bash"> yum install wget -y </span>    <span class="hljs-keyword">RUN</span><span class="language-bash"> yum install nginx -y </span>    <span class="hljs-keyword">COPY</span><span class="language-bash"> index.html /usr/share/nginx/html/ </span>    <span class="hljs-keyword">EXPOSE</span> <span class="hljs-number">80</span>     <span class="hljs-keyword">ENTRYPOINT</span><span class="language-bash"> [<span class="hljs-string">"/usr/sbin/nginx"</span>,<span class="hljs-string">"-g"</span>,<span class="hljs-string">"daemon off;"</span>] </span>    vim index.html     <html>     <head>   （<span class="hljs-number">9</span>）<span class="hljs-keyword">VOLUME</span><span class="language-bash"> </span>定义匿名数据卷。在启动容器时忘记挂载数据卷，会自动挂载到匿名卷。 作用： <span class="hljs-number">1</span>、避免重要的数据，因容器重启而丢失，这是非常致命的。 <span class="hljs-number">2</span>、避免容器不断变大。 格式： <span class="hljs-keyword">VOLUME</span><span class="language-bash"> [<span class="hljs-string">"<路径 1>"</span>, <span class="hljs-string">"<路径 2>"</span>...] </span><span class="hljs-keyword">VOLUME</span><span class="language-bash"> <路径> </span>在启动容器 docker <span class="hljs-keyword">run</span><span class="language-bash"> 的时候，我们可以通过 -v 参数修改挂载点。 </span> <span class="hljs-keyword">VOLUME</span><span class="language-bash"> [“/data”] </span> （<span class="hljs-number">10</span>）<span class="hljs-keyword">WORKDIR</span><span class="language-bash"> </span>    指定工作目录。用 <span class="hljs-keyword">WORKDIR</span><span class="language-bash"> 指定的工作目录，会在构建镜像的每一层中都存在。（WORKDIR 指定的工作</span>    目录，必须是提前创建好的）。     docker build 构建镜像过程中的，每一个 <span class="hljs-keyword">RUN</span><span class="language-bash"> 命令都是新建的一层。只有通过 WORKDIR 创建的目录才</span>    会一直存在。     格式：     <span class="hljs-keyword">WORKDIR</span><span class="language-bash"> <工作目录路径> </span>    <span class="hljs-keyword">WORKDIR</span><span class="language-bash"> /path/to/workdir </span>    （填写绝对路径）  (<span class="hljs-number">11</span> )<span class="hljs-keyword">ENV</span>     设置环境变量     <span class="hljs-keyword">ENV</span> <key> <value>     <span class="hljs-keyword">ENV</span> <key>=<value>...     以下示例设置 NODE_VERSION =<span class="hljs-number">6.6</span>.<span class="hljs-number">6</span>， 在后续的指令中可以通过 $NODE_VERSION 引用：     <span class="hljs-keyword">ENV</span> NODE_VERSION <span class="hljs-number">6.6</span>.<span class="hljs-number">6</span>     <span class="hljs-keyword">RUN</span><span class="language-bash"> curl -SLO <span class="hljs-string">"https://nodejs.org/dist/v<span class="hljs-variable">$NODE_VERSION</span>/node-v<span class="hljs-variable">$NODE_VERSION</span>-linux-x64.tar.xz"</span> </span>    \      && curl -SLO <span class="hljs-string">"https://nodejs.org/dist/v$NODE_VERSION/SHASUMS256.txt.asc"</span>  （<span class="hljs-number">12</span>）<span class="hljs-keyword">USER</span>     用于指定执行后续命令的用户和用户组，这边只是切换后续命令执行的用户（用户和用户组必须提前已    经存在）。     格式：     <span class="hljs-keyword">USER</span> <用户名>[:<用户组>]     <span class="hljs-keyword">USER</span> daemon     <span class="hljs-keyword">USER</span> nginx     <span class="hljs-keyword">USER</span> <span class="hljs-keyword">user</span>     <span class="hljs-keyword">USER</span> uid     <span class="hljs-keyword">USER</span> <span class="hljs-keyword">user</span>:group     <span class="hljs-keyword">USER</span> uid:gid     <span class="hljs-keyword">USER</span> <span class="hljs-keyword">user</span>:gid     <span class="hljs-keyword">USER</span> uid:group  （<span class="hljs-number">13</span>）<span class="hljs-keyword">ONBUILD</span>     用于延迟构建命令的执行。简单的说，就是 Dockerfile 里用 <span class="hljs-keyword">ONBUILD</span> 指定的命令，在本次构建镜    像的过程中不会执行（假设镜像为 test-build）。当有新的 Dockerfile 使用了之前构建的镜像 <span class="hljs-keyword">FROM</span>     test-build ，这时执行新镜像的 Dockerfile 构建时候，会执行 test-build 的 Dockerfile 里的     <span class="hljs-keyword">ONBUILD</span> 指定的命令。     格式：     <span class="hljs-keyword">ONBUILD</span> <其它指令>     为镜像添加触发器当一个镜像被其他镜像作为基础镜像时需要写上 OBNBUILD 会在构建时插入触发器指令（<span class="hljs-number">14</span>）<span class="hljs-keyword">LABEL</span><span class="language-bash"> </span>    <span class="hljs-keyword">LABEL</span><span class="language-bash"> 指令用来给镜像添加一些元数据（metadata），以键值对的形式，语法格式如下：</span>    <span class="hljs-keyword">LABEL</span><span class="language-bash"> <key>=<value> <key>=<value> <key>=<value> ...</span>    比如我们可以添加镜像的作者：    <span class="hljs-keyword">LABEL</span><span class="language-bash"> org.opencontainers.image.authors=<span class="hljs-string">"xianchao"</span></span> （<span class="hljs-number">15</span>）<span class="hljs-keyword">HEALTHCHECK</span><span class="language-bash"> </span>    用于指定某个程序或者指令来监控 docker 容器服务的运行状态。    格式：    <span class="hljs-keyword">HEALTHCHECK</span><span class="language-bash"> [选项] CMD <命令>：设置检查容器健康状况的命令</span>    <span class="hljs-keyword">HEALTHCHECK</span><span class="language-bash"> NONE：如果基础镜像有健康检查指令，使用这行可以屏蔽掉其健康检查指令</span>    <span class="hljs-keyword">HEALTHCHECK</span><span class="language-bash"> [选项] CMD <命令> : 这边 CMD 后面跟随的命令使用，可以参考 CMD 的用法。</span>        选项    --interval=<间隔> ：两次健康检查的间隔，默认为 <span class="hljs-number">30</span> 秒；    --timeout=<时长> ：健康检查命令运行超时时间，如果超过这个时间，本次健康检查就被视为失败，默认 <span class="hljs-number">30</span> 秒；    --retries=<次数> ：当连续失败指定次数后，则将容器状态视为 unhealthy ，默认 <span class="hljs-number">3</span> 次        举例：    <span class="hljs-keyword">FROM</span> nginx<span class="hljs-keyword">RUN</span><span class="language-bash"> apt-get update && apt-get install -y curl && <span class="hljs-built_in">rm</span> -rf /var/lib/apt/lists/*</span><span class="hljs-keyword">HEALTHCHECK</span><span class="language-bash"> --interval=5s --<span class="hljs-built_in">timeout</span>=3s \</span><span class="language-bash">CMD curl -fs http://localhost/ || <span class="hljs-built_in">exit</span> 1</span><span class="hljs-comment"># 如果返回 1 则退出</span><span class="hljs-comment"># 执行 docker build</span>$ docker build -f HEALTHCHEK.dockerifle -t myweb .（<span class="hljs-number">16</span>）<span class="hljs-keyword">ARG</span>     构建参数，与 <span class="hljs-keyword">ENV</span> 作用一至。不过作用域不一样。<span class="hljs-keyword">ARG</span> 设置的环境变量仅对 Dockerfile 内有效，也就    是说只有 docker build 的过程中有效，构建好的镜像内不存在此环境变量。    构建命令 docker build 中可以用 --build-<span class="hljs-keyword">arg</span> <参数名>=<值> 来覆盖。    格式：    <span class="hljs-keyword">ARG</span> <参数名>[=<默认值>]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="五、将编译后的文件进行构建"><a href="#五、将编译后的文件进行构建" class="headerlink" title="五、将编译后的文件进行构建"></a>五、将编译后的文件进行构建</h1><h2 id="1、将go代码进行构建镜像"><a href="#1、将go代码进行构建镜像" class="headerlink" title="1、将go代码进行构建镜像"></a>1、将go代码进行构建镜像</h2><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">把 Go 代码基于 dockerfile 做成镜像 1、linux 机器安装 go（我用的是 centos7.9 操作系统） [root@master1 ~]# yum install go -y 2、创建源码文件 生产环境代码是开发人员写的，运维不用写 [root@master1 ~]# mkdir test [root@master1 ~]# cd test/ [root@master1 test]# cat main.go package main  import (  "net/http"   "github.com/gin-gonic/gin" )  func statusOKHandler(c *gin.Context) &#123;  c.JSON(http.StatusOK, gin.H&#123;"status": "success~welcome to study"&#125;) &#125;  func versionHandler(c *gin.Context) &#123;  c.JSON(http.StatusOK, gin.H&#123;"version": "v1.1 版本"&#125;) &#125;  func main() &#123;  router := gin.New()  router.Use(gin.Recovery())  router.GET("/", statusOKHandler)  router.GET("/version", versionHandler)  router.Run(":8080") &#125;  3、Go mod 初始化项目 [root@master1 test]# go mod init test #设置代理 [root@master1 test]# go env -w GOPROXY=https://goproxy.cn,direct [root@master1 test]# go mod tidy #构建源码 [root@master1 test]# CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o k8s-demo main.go <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>构建镜像</p><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext"> 4、编写 dockerfile 文件 [root@master1 test]# cat Dockerfile FROM alpine WORKDIR /data/app ADD k8s-demo /data/app/ CMD ["/bin/sh","-c","./k8s-demo"]  5、构建镜像 [root@master1 test]# docker build -t xianchao/k8sdemo:v1 .  6、运行镜像 [root@master1 test]# docker run -d --name go -p 30180:8080 xianchao/k8sdemo:v1  7、访问 go 容器 可以在根物理机同网段的其他机器或者浏览器访问 master1 机器的 ip:30180，就可以把请求代理到 go 容器了<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2、将java代码构建成镜像"><a href="#2、将java代码构建成镜像" class="headerlink" title="2、将java代码构建成镜像"></a>2、将java代码构建成镜像</h2><pre class="line-numbers language-hljs dockerfile"><span class="hljs-keyword">FROM</span> java:<span class="hljs-number"><code class="language-hljs dockerfile"><span class="hljs-keyword">FROM</span> java:<span class="hljs-number">8</span><span class="hljs-comment">#COPY target/dockerk8sdome-0.0.1-SNAPSHOT.jar dockerk8sdome.jar</span><span class="hljs-keyword">COPY</span><span class="language-bash"> dockerk8sdome-0.0.1-SNAPSHOT.jar dockerk8sdome.jar</span><span class="hljs-comment"># 开放端口</span><span class="hljs-keyword">EXPOSE</span> <span class="hljs-number">8080</span><span class="hljs-keyword">ENTRYPOINT</span><span class="language-bash"> [<span class="hljs-string">"java"</span>,<span class="hljs-string">"-jar"</span>,<span class="hljs-string">"dockerk8sdome.jar"</span>]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="六、Docker-数据卷管理"><a href="#六、Docker-数据卷管理" class="headerlink" title="六、Docker 数据卷管理"></a>六、Docker 数据卷管理</h1><p>什么是数据卷？ </p><p>​简单来说：就是对docker的容器与主机同步数据，实现容器的数据持久化操作。</p><p>​数据卷是经过特殊设计的目录，可以绕过联合文件系统（UFS），为一个或者多个容器提供访问，数据卷设计的目的，在于数据的永久存储，它完全独立于容器的生存周期，因此，docker 不会在容器删除时删除其挂载的数据卷，也不会存在类似的垃圾收集机制，对容器引用的数据卷进行处理，同一个数据卷可以只支持多个容器的访问。</p><p>数据卷的特点： </p><ol><li>数据卷在容器启动时初始化，如果容器使用的镜像在挂载点包含了数据，这些数据会被拷贝到新初始化的数据卷中 </li><li>数据卷可以在容器之间共享和重用 </li><li>可以对数据卷里的内容直接进行修改 </li><li>数据卷的变化不会影像镜像的更新 </li><li>卷会一直存在，即使挂载数据卷的容器已经被删除</li></ol><h2 id="1、为容器添加数据卷"><a href="#1、为容器添加数据卷" class="headerlink" title="1、为容器添加数据卷"></a><strong>1、为容器添加数据卷</strong></h2><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">docker run -v /datavolume:/data -it centos /bin/bash# 注：~/datavolume 为宿主机目录，/data 为 docker 启动的 volume 容器的里的目录,这样在宿主机的/datavolume 目录下创建的数据就会同步到容器的/data 目录下（1）为数据卷添加访问权限 docker run --name volume1 -v ~/datavolume1:/data:ro -itd centos /bin/bash<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2、Docker-的数据卷容器共享"><a href="#2、Docker-的数据卷容器共享" class="headerlink" title="2、Docker 的数据卷容器共享"></a>2、Docker 的数据卷容器共享</h2><p>什么是数据卷容器：<br>命名的容器挂载数据卷，其他容器通过挂载这个容器实现数据共享，挂载数据卷的容器，就叫做数据卷容器 </p><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext"># 挂载数据卷容器的方法 docker run --volumes-from [container name] 例： 这个--volumes-from后面跟的是容器的名称。docker run --name data-volume -itd volume（volume 这个镜像是上面创建的带两个数据卷/datavolume3 和/ddatavolume6 的镜像）docker run --name data-volume2 --volumes-from data-volume -itd centos /bin/bash<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3、docker-数据卷的备份和还原"><a href="#3、docker-数据卷的备份和还原" class="headerlink" title="3、docker 数据卷的备份和还原"></a>3、docker 数据卷的备份和还原</h2><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext"># 数据备份方法：docker run --volumes-from [镜像--name名字] -v $(pwd):/backup [镜像名称] tar czvf /backup/[包名称].tar [container data volume]例子： docker run --volumes-from data-volume2 -v /root/backup:/backup --name datavolume-copy centos tar zcvf /backup/data-volume2.tar.gz /datavolume6docker run -tid --rm --volumes-from appname-minio4-1 -v $(pwd):/backup minio/minio:latest tar cvf /backup/backup.tar /data/minio/data4 # 数据还原方法： docker run --volumes-from [container name] -v $(pwd):/backup centos tar xzvf /backup/backup.tar.gz [container data volume]例： docker exec -it data-volume2 /bin/bash cd /datavolume6 rm -rf lucky.txt  docker run --volumes-from data-volume2 -v /root/backup/:/backup centos tar zxvf /backup/data-volume2.tar.gz -C /datavolume6  docker exec -it data-volum2 /bin/bash cd /datavolum6 # 可以看到还原后的数据<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="七、docker-容器的网络模式"><a href="#七、docker-容器的网络模式" class="headerlink" title="七、docker 容器的网络模式"></a>七、docker 容器的网络模式</h1><p>docker run 创建 docker 容器时，可以用–net 选项指定容器的网络模式，Docker 有以下 4 种网络模式： </p><p> bridge 模式：使–net &#x3D;bridge 指定，默认设置； </p><p> host 模式：使–net &#x3D;host 指定； </p><p> none 模式：使–net &#x3D;none 指定； </p><p> container 模式：使用–net &#x3D;container:NAME orID 指定。 </p><h2 id="1、安装网桥管理工具："><a href="#1、安装网桥管理工具：" class="headerlink" title="1、安装网桥管理工具："></a>1、安装网桥管理工具：</h2><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">yum install bridge-utils -y brctl show 可以查看到有一个 docker0 的网桥设备，下面有很多接口，每个接口都表示一个启动的docker 容器，因为我在 docker 上启动了很多容器，所以 interfaces 较多<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="2、-docker-link-设置网络别名"><a href="#2、-docker-link-设置网络别名" class="headerlink" title="2、 docker link 设置网络别名"></a>2、 docker link 设置网络别名</h2><p>可以给容器起一个代号，这样可以直接以代号访问，避免了容器重启 ip 变化带来的问题 –link </p><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">docker run --link=[CONTAINER_NAME]:[ALIAS] [IMAGE][COMMAND] 例： 1.启动一个 test3 容器 docker run --name test3 -itd inter-image /bin/bash   2.启动一个 test5 容器，--link 做链接，那么当我们重新启动 test3 容器时，就算 ip 变了，也没关系，我们可以在 test5 上 ping 别名 webtest docker run --name test5 -itd --link=test3:webtest inter-image /bin/bash  3.test3 和 test5 的 ip 分别是 172.17.0.22 和 172.17.0.24  4.重启 test3 容器 docker restart test3 发现 ip 变成了 172.17.0.25  5.进入到 test5 容器 docker exec -it test5 /bin/bash  ping test3 容器的 ip 别名 webtest 可以 ping 通，尽管 test3 容器的 ip 变了也可以通<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3、none-模式"><a href="#3、none-模式" class="headerlink" title="3、none 模式"></a>3、<strong>none 模式</strong></h2><p>Docker 网络 none 模式是指创建的容器没有网络地址，只有 lo 网卡</p><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">docker run -itd --name none --net=none --privileged=true centosdocker exec -it none /bin/bash[root@05dbf3f2daaf /]# ip addr #只有本地 lo 地址  1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000  link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00  inet 127.0.0.1/8 scope host lo  valid_lft forever preferred_lft forever<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4、container-模式"><a href="#4、container-模式" class="headerlink" title="4、container 模式"></a><strong>4、container 模式</strong></h2><p>​简单来说，指定一个已存在的容器，复制他的网络模式。</p><p>​Docker 网络 container 模式是指，创建新容器的时候，通过–net container 参数，指定其和已经存在的某个容器共享一个 Network Namespace。</p><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">#和已经存在的 none 容器共享网络[root@master1 ~]# docker run --name container2 --net=container:none -it --privileged=true centos[root@05dbf3f2daaf /]# ip addr1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="5、-bridge-模式"><a href="#5、-bridge-模式" class="headerlink" title="5、 bridge 模式"></a>5、 <strong>bridge 模式</strong></h2><p>​默认选择 bridge 的情况下，容器启动后会通过 DHCP 获取一个地址</p><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext"># 创建桥接网络[root@master1~]# docker run --name bridge -it --privileged=true centos bash[root@a131580fb605 /]# ip addr1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever64: eth0@if65: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default  link/ether 02:42:ac:11:00:0d brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.17.0.13/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4、host-模式"><a href="#4、host-模式" class="headerlink" title="4、host 模式"></a><strong>4、host 模式</strong></h2><p>Docker 网络 host 模式是指共享宿主机的网络</p><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext"># 共享宿主机网络[root@master1~]# docker run --name host -it --net=host --privileged=true centos bash<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h1 id="docker-资源配额"><a href="#docker-资源配额" class="headerlink" title="docker 资源配额"></a><strong>docker 资源配额</strong></h1><p>Docker 通过 cgroup 来控制容器使用的资源限制，可以对 docker 限制的资源包括 CPU、内存、磁盘</p><h2 id="1、指定-docker-容器可以使用的-cpu-份额"><a href="#1、指定-docker-容器可以使用的-cpu-份额" class="headerlink" title="1、指定 docker 容器可以使用的 cpu 份额"></a><strong>1、指定 docker 容器可以使用的 cpu 份额</strong></h2><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext"># 说白了，给他一个弹性的加权值，多个容器会进行分抢cup份额，权重越高，占用份额越多docker run -it --cpu-shares 1024 centos /bin/bash# CPU shares (relative weight) 在创建容器时指定容器所使用的 CPU 份额值。cpu-shares 的值不能保证可以获得 1 个 vcpu 或者多少 GHz 的 CPU 资源，仅仅只是一个弹性的加权值。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>例： 两个容器 A、B 的 cpu 份额分别为 1000 和 500，结果会怎么样？</p><p>**情况 1：**A 和 B 正常运行，占用同一个 CPU，在 cpu 进行时间片分配的时候，容器 A 比容器 B 多一倍</p><p>的机会获得 CPU 的时间片。 </p><p>**情况 2：**分配的结果取决于当时其他容器的运行状态。比如容器 A 的进程一直是空闲的，那么容器 B是可以获取比容器 A 更多的 CPU 时间片的； 比如主机上只运行了一个容器，即使它的 cpu 份额只有50，它也可以独占整个主机的 cpu 资源。</p><p>cgroups 只在多个容器同时争抢同一个 cpu 资源时，cpu 配额才会生效。因此，无法单纯根据某个容器的 cpu 份额来确定有多少 cpu 资源分配给它，资源分配结果取决于同时运行的其他容器的 cpu 分配和容器中进程运行情况</p><p>例 1：给容器实例分配 512 权重的 cpu 使用份额 </p><p>参数： –cpu-shares 512 </p><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">[root@master1 ~]# docker run -it --cpu-shares 512 centos /bin/bash [root@df176dd75bd4 /]# cat /sys/fs/cgroup/cpu/cpu.shares#查看结果： 512<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注：稍后，我们启动多个容器，测试一下是不是只能使用 512 份额的 cpu 资源。单独一个容器，看不出来使用的 cpu 的比例。 因没有 docker 实例同此 docker 实例竞争。</p><p>总结：</p><p>​通过-c 设置的 cpu share 并不是 CPU 资源的绝对数量，而是一个相对的权重值。某个容器最终能分配到的 CPU 资源取决于它的 cpu share 占所有容器 cpu share 总和的比例。通过 cpu share 可以设置容器使用 CPU 的优先级</p><p>比如在 host 中启动了两个容器：</p><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">docker run --name "container_A" -c 1024 ubuntudocker run --name "container_B" -c 512 ubuntu<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>container_A 的 cpu share 1024，是 container_B 的两倍。当两个容器都需要 CPU 资源时，container_A 可以得到的 CPU 是 container_B 的两倍。</p><p>需要注意的是，这种按权重分配 CPU 只会发生在 CPU 资源紧张的情况下。如果 container_A 处于空闲状态，为了充分利用 CPU 资源，container_B 也可以分配到全部可用的 CPU。</p><h2 id="2、CPU-core-核心控制"><a href="#2、CPU-core-核心控制" class="headerlink" title="2、CPU core 核心控制"></a><strong>2、CPU core 核心控制</strong></h2><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">docker run -itd --name docker10 --cpuset-cpus 0,1 --cpu-shares 512 centos /bin/bash<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>参数：–cpuset 可以绑定 CPU </p><p>​对多核 CPU 的服务器，docker 还可以控制容器运行限定使用哪些 cpu 内核和内存节点，即使用–cpuset-cpus 和–cpuset-mems 参数。对具有 NUMA 拓扑（具有多 CPU、多内存节点）的服务器尤其有用，可以对需要高性能计算的容器进行性能最优的配置。如果服务器只有一个内存节点，则–cpuset-mems 的配置基本上不会有明显效果。</p><p>扩展： </p><p>服务器架构一般分： SMP、NUMA、MPP 体系结构介绍 </p><p>从系统架构来看，目前的商用服务器大体可以分为三类： </p><ol><li><p>即对称多处理器结构(SMP ： Symmetric Multi-Processor) 例： x86 服务器，双路服务器。主板上有两个物理 cpu </p></li><li><p>非一致存储访问结构 (NUMA ： Non-Uniform Memory Access) 例： IBM 小型机pSeries 690 </p></li><li><p>海量并行处理结构 (MPP ： Massive ParallelProcessing) 。 例： 大型机 Z14</p></li></ol><p>知识点：</p><p><strong>CPU 配额控制参数的混合使用</strong> </p><p>容器 A 和容器 B 配置上 cpuset-cpus 值并都绑定到同一个 cpu 上，然后同时抢占 cpu 资源，就可以</p><p>看出效果了。 </p><p>例 1：测试 cpu-shares 和 cpuset-cpus 混合使用运行效果，就需要一个压缩力测试工具 stress 来让</p><p>容器实例把 cpu 跑满</p><p><strong>如何把 cpu 跑满？</strong></p><p>如何把 4 核心的 cpu 中第一和第三核心跑满？可以运行 stress，然后使用 taskset 绑定一下 cpu。 </p><p>先扩展：stress 命令 </p><p>概述：linux 系统压力测试软件 Stress 。</p><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">[root@master1 ~]# yum install -y epel-release [root@master1 ~]# yum install stress -y stress 参数解释 -? 显示帮助信息 -v 显示版本号 -q 不显示运行信息 -n 显示已完成的指令情况 -t --timeout N 指定运行 N 秒后停止 --backoff N 等待 N 微妙后开始运行 -c 产生 n 个进程 ：每个进程都反复不停的计算随机数的平方根，测试 cpu -i 产生 n 个进程 ：每个进程反复调用 sync()，sync()用于将内存上的内容写到硬盘上，测试磁盘 io -m --vm n 产生 n 个进程,每个进程不断调用内存分配 malloc（）和内存释放 free（）函数 ，测试内存 --vm-bytes B 指定 malloc 时内存的字节数 （默认 256MB） --vm-hang N 指定在 free 栈的秒数 -d --hadd n 产生 n 个执行 write 和 unlink 函数的进程 -hadd-bytes B 指定写的字节数 --hadd-noclean 不 unlink 注：时间单位可以为秒 s，分 m，小时 h，天 d，年 y，文件大小单位可以为 K，M，G  例 1：产生 2 个 cpu 进程，2 个 io 进程，20 秒后停止运行 [root@master1]# stress -c 2 -i 2 --verbose --timeout 20s #如果执行时间为分钟，改 20s 为 1m  查看：top例 1：测试 cpuset-cpus 和 cpu-shares 混合使用运行效果，就需要一个压缩力测试工具 stress 来让容器实例把 cpu 跑满。 当跑满后，会不会去其他 cpu 上运行。 如果没有在其他 cpu 上运行，说明cgroup 资源限制成功。 实例 3：创建两个容器实例:docker10 和 docker20。 让 docker10 和 docker20 只运行在 cpu0 和cpu1 上，最终测试一下 docker10 和 docker20 使用 cpu 的百分比。测试 1： 进入 docker10，使用 stress 测试进程是不是只在 cpu0,1 上运行： [root@master1 ~]# docker run -it --name docker10 centos   /bin/bash [root@d1a431815090 /]# yum install -y epel-release #安装 epel 扩展源 [root@d1a431815090 /]# yum install stress -y #安装 stress 命令 [root@d1a431815090 /]# stress -c 2 -v -t 10m #运行 2 个进程，把两个 cpu 占满 在物理机另外一个虚拟终端上运行 top 命令，按 1 快捷键，查看每个 cpu 使用情况： 测试 2： 然后进入 docker20，使用 stress 测试进程是不是只在 cpu0,1 上运行，且 docker20 上运行的 stress 使用 cpu 百分比是 docker10 的 2 倍 [root@master1 ~]# docker run -it --name docker20 centos /bin/bash [root@d1a431815090 /]# yum install -y epel-release #安装 epel 扩展源 [root@d1a431815090 /]# yum install stress -y [root@f24e75bca5c0 /]# stress -c 2 -v -t 10m  在另外一个虚拟终端上运行 top 命令，按 1 快捷键，查看每个 cpu 使用情况：注：两个容器只在 cpu0,1 上运行，说明 cpu 绑定限制成功。而 docker20 是 docker10 使用 cpu 的 2倍。说明--cpu-shares 限制资源成功。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3、docker-容器控制内存"><a href="#3、docker-容器控制内存" class="headerlink" title="3、docker 容器控制内存"></a><strong>3、docker 容器控制内存</strong></h2><p>Docker 提供参数-m, –memory&#x3D;””限制容器的内存使用量。 </p><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">例 1：允许容器使用的内存上限为 128M： [root@master1 ~]# docker run -it -m 128m centos 查看： [root@40bf29765691 /]# cat /sys/fs/cgroup/memory/memory.limit_in_bytes 134217728 注：也可以使用 tress 进行测试，到现在，我可以限制 docker 实例使用 cpu 的核心数和权重，可以限制内存大小。 例 2：创建一个 docker，只使用 2 个 cpu 核心，只能使用 128M 内存 [root@master1 ~]# docker run -it --cpuset-cpus 0,1 -m 128m centos<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4、docker-容器控制-IO"><a href="#4、docker-容器控制-IO" class="headerlink" title="4、docker 容器控制 IO"></a><strong>4、docker 容器控制 IO</strong></h2><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">[root@master1 ~]# docker run --help | grep write-b  --device-write-bps value Limit write rate (bytes per second) to a device (default []) #限制此设备上的写速度（bytes per second），单位可以是 kb、mb 或者 gb。 --device-read-bps value #限制此设备上的读速度（bytes per second），单位可以是 kb、mb 或者 gb。 情景：防止某个 Docker 容器吃光你的磁盘 I / O 资源 例 1：限制容器实例对硬盘的最高写入速度设定为 2MB/s。 --device 参数：将主机设备添加到容器 [root@master1 ~]# mkdir -p /var/www/html/ [root@master1 ~]# docker run -it -v /var/www/html/:/var/www/html --device /dev/sda:/dev/sda --device-write-bps /dev/sda:2mb centos /bin/bash [root@bd79042dbdc9 /]# time dd if=/dev/sda of=/var/www/html/test.out bs=2M count=50 oflag=direct,nonblock 注：dd 参数： direct：读写数据采用直接 IO 方式，不走缓存。直接从内存写硬盘上。 nonblock：读写数据采用非阻塞 IO 方式，优先写 dd 命令的数据 50+0 records in 50+0 records out 52428800 bytes (52 MB) copied, 50.1831 s, 2.0 MB/s  real 0m50.201s user 0m0.001s sys 0m0.303s 注： 发现 1 秒写 2M。 限制成功。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="5、docker-容器运行结束自动释放资源"><a href="#5、docker-容器运行结束自动释放资源" class="headerlink" title="5、docker 容器运行结束自动释放资源"></a><strong>5、docker 容器运行结束自动释放资源</strong></h2><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">[root@master1 ~]# docker run --help | grep rm  --rm 参数： Automatically remove the container when it exits 作用：当容器命令运行结束后，自动删除容器，自动释放资源 例： [root@master1 ~]# docker run -it --rm --name xianchao centos sleep 6 物理上查看： [root@master1 ~]# docker ps -a | grep xianchao 6c75a9317a6b centos "sleep 6" 6 seconds ago Up 4 seconds mk 等 5s 后，再查看： [root@master1 ~]# docker ps | grep xianchao #自动删除了<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="八、docker-私有镜像仓库-harbor"><a href="#八、docker-私有镜像仓库-harbor" class="headerlink" title="八、docker 私有镜像仓库 harbor"></a><strong>八、docker 私有镜像仓库 harbor</strong></h1><p>Harbor 介绍</p><p>​Docker 容器应用的开发和运行离不开可靠的镜像管理，虽然 Docker 官方也提供了公共的镜像仓库，但是从安全和效率等方面考虑，部署我们私有环境内的 Registry 也是非常必要的。Harbor 是由 VMware公司开源的企业级的 Docker Registry 管理项目，它包括权限管理(RBAC)、LDAP、日志审核、管理界面、自我注册、镜像复制和中文支持等功能。</p><p>官网地址：<a href="https://github.com/goharbor/harbor">https://github.com/goharbor/harbor</a></p><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">安装 harbor 的机器，主机名设置成 harbor机器需要的内存至少要 2G，我分配的是 4G机器 ip：192.168.40.1814vCPU/4G 内存/100G 硬盘<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="1、安装证书："><a href="#1、安装证书：" class="headerlink" title="1、安装证书："></a>1、安装证书：</h2><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">为 Harbor 自签发证书 [root@192 ~]# hostnamectl set-hostname harbor && bash[root@harbor ~]# mkdir /data/ssl -p[root@harbor ~]# cd /data/ssl/生成 ca 证书：[root@harbor ssl]# openssl genrsa -out ca.key 3072#生成一个 3072 位的 key，也就是私钥 [root@harbor ssl]# openssl req -new -x509 -days 3650 -key ca.key -out ca.pem#生成一个数字证书 ca.pem，3650 表示证书的有效时间是 3 年，按箭头提示填写即可，没有箭头标注的为空：Country Name (2 letter code) [XX]:CHState or Province Name (full name) []:BJLocality Name (eg, city) [Default City]:BJOrganization Name (eg, company) [Default Company Ltd]:Organizational Unit Name (eg, section) []:Common Name (eg, your name or your server&#x27;s hostname) []:Email Address []:生成域名的证书： [root@harbor ssl]# openssl genrsa -out harbor.key 3072#生成一个 3072 位的 key，也就是私钥[root@harbor ssl]# openssl req -new -key harbor.key -out harbor.csr#生成一个证书请求，一会签发证书时需要的，标箭头的按提示填写，没有箭头标注的为空-----Country Name (2 letter code) [XX]:CHState or Province Name (full name) []:BJLocality Name (eg, city) [Default City]:BJOrganization Name (eg, company) [Default Company Ltd]:Organizational Unit Name (eg, section) []:Common Name (eg, your name or your server&#x27;s hostname) []:harborEmail Address []:Please enter the following &#x27;extra&#x27; attributesto be sent with your certificate requestA challenge password []:An optional company name []:签发证书：[root@harbor ssl]# openssl x509 -req -in harbor.csr -CA ca.pem -CAkey ca.key -CAcreateserial -out harbor.pem -days 3650显示如下，说明证书签发好了：signature oksubject=/C=CH/ST=BJ/L=BJ/0=Default Company Ltd/CN=harborGetting CA Private Key<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2、安装-Harbor"><a href="#2、安装-Harbor" class="headerlink" title="2、安装 Harbor"></a><strong>2、安装 Harbor</strong></h2><p>需要docker环境。</p><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">#创建安装目录[root@harbor ssl]# mkdir /data/install -p && cd /data/install/##安装 harbor/data/ssl 目录下有如下文件：ca.key ca.pem ca.srl harbor.csr harbor.key harbor.pem[root@harbor install]# cd /data/install/#把 harbor 的离线包 harbor-offline-installer-v2.3.0-rc3.tgz 上传到这个目录，离线包在课件里提供了#下载 harbor 离线包的地址：https://github.com/goharbor/harbor/releases/tag/解压：[root@harbor install]# tar zxvf harbor-offline-installer-v2.3.0-rc3.tgz[root@harbor install]# cd harbor[root@harbor harbor]# cp harbor.yml.tmpl harbor.yml[root@harbor harbor]# vim harbor.yml修改配置文件：hostname: harbor #修改 hostname，跟上面签发的证书域名保持一致hostname: harbor#协议用 httpscertificate: /data/ssl/harbor.pemprivate_key: /data/ssl/harbor.key#邮件和 ldap 不需要配置，在 harbor 的 web 界面可以配置#其他配置采用默认即可#修改之后保存退出注：harbor 默认的账号密码：admin/Harbor12345=================================================================================#安装 docker-compose#上传课件里的 docker-compose-Linux-x86_64 文件到 harbor 机器。这一步不需要联网下载镜像了，省事[root@harbor harbor]# mv docker-compose-Linux-x86_64.64 /usr/bin/docker-compose[root@harbor harbor]# chmod +x /usr/bin/docker-compose注： docker-compose 项目是 Docker 官方的开源项目，负责实现对 Docker 容器集群的快速编排。Docker-Compose 的工程配置文件默认为 docker-compose.yml，Docker-Compose 运行目录下的必要有一个docker-compose.yml。docker-compose 可以管理多个 docker 实例。安装 harbor 需要的离线镜像包 docker-harbor-2-3-0.tar.gz 在课件，可上传到 harbor 机器，通过docker load -i 解压[root@harbor install]# docker load -i docker-harbor-2-3-0.tar.gz [root@harbor install]# cd /data/install/harbor# 配置私有仓库地址[root@master1 ~]# vim /etc/docker/daemon.json&#123; "registry-mirrors":["https://rsbud4vc.mirror.aliyuncs.com","https://registry.docker-cn.com","https://docker.mirrors.ustc.edu.cn","https://dockerhub.azk8s.cn","http://hub-mirror.c.163.com","http://qtid6917.mirror.aliyuncs.com", "https://rncxm540.mirror.aliyuncs.com"], "insecure-registries": ["192.168.162.30","harbor"], "exec-opts": ["native.cgroupdriver=systemd"]&#125; #注意：#配置新增加了一行内容如下："insecure-registries":["192.168.162.30"], #上面增加的内容表示我们内网访问 harbor 的时候走的是 http，192.168.162.30 是安装 harbor 机器的 ip# 修改配置之后使配置生效：[root@master1 ~]# systemctl daemon-reload && systemctl restart docker# 查看 docker 是否启动成功[root@master1 ~]# systemctl status docker#显示如下，说明启动成功：Active: active (running) # 安装[root@harbor harbor]# ./install.sh#在自己电脑修改 hosts 文件#在 hosts 文件添加如下一行，然后保存即可,ip写你本机的192.168.162.30 harbor# 虚拟机也需要配置一下vim /etc/hosts192.168.162.30 harbor扩展：如何停掉 harbor：[root@harbor harbor]# cd /data/install/harbor[root@harbor harbor]# docker-compose stop 如何启动 harbor：[root@harbor harbor]# cd /data/install/harbor[root@harbor harbor]# docker-compose start如果 docker-compose start 启动 harbor 之后，还是访问不了，那就需要重启虚拟机<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3、Harbor-图像化界面使用说明"><a href="#3、Harbor-图像化界面使用说明" class="headerlink" title="3、Harbor 图像化界面使用说明"></a><strong>3、Harbor 图像化界面使用说明</strong></h2><p>在浏览器输入：</p><p><a href="https://harbor/">https://harbor</a></p><p>账号：admin</p><p>密码：Harbor12345</p><h2 id="4、测试使用-harbor-私有镜像仓库"><a href="#4、测试使用-harbor-私有镜像仓库" class="headerlink" title="4、测试使用 harbor 私有镜像仓库"></a><strong>4、测试使用 harbor 私有镜像仓库</strong></h2><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">#登录 harbor：[root@master1]# docker login 192.168.162.30Username：adminPassword: Harbor12345输入账号密码之后看到如下，说明登录成功了：Login Succeeded# 错误看下面#导入 tomcat 镜像，tomcat.tar.gz 在课件里[root@master1 ~]# docker load -i tomcat.tar.gz#把 tomcat 镜像打标签，这里需要在horbor上新建一个项目test[root@master1 ~]# docker tag tomcat:latest 192.168.162.30/test/tomcat:v1# 执行上面命令就会把 192.168.40.181/test/tomcat:v1 上传到 harbor 里的 test 项目下[root@master1 ~]# docker push 192.168.162.30/test/tomcat:v1执行上面命令就会把 192.168.40.181/test/tomcat:v1 上传到 harbor 里的 test 项目下 # 从 harbor 仓库下载镜像 # 在 master1 机器上删除镜像[root@master1 ~]# docker rmi -f 192.168.162.30/test/tomcat:v1# 拉取镜像[root@master1 ~]#docker pull 192.168.162.30/test/tomcat:v1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="5、docker-login-192-168-162-30-错误问题"><a href="#5、docker-login-192-168-162-30-错误问题" class="headerlink" title="5、docker login 192.168.162.30 错误问题"></a>5、docker login 192.168.162.30 错误问题</h2><p>Error response from daemon: Get “<a href="https://192.168.162.30/v2/">https://192.168.162.30:443/v2/</a>“: Get “<a href="https://harbor/service/token?account=admin&client_id=docker&offline_token=true&service=harbor-registry">https://harbor/service/token?account=admin&amp;client_id=docker&amp;offline_token=true&amp;service=harbor-registry</a>“: dial tcp: lookup harbor on 114.114.114.114:53: no such host</p><p>解决：</p><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">修改 /etc/hostsvim /etc/hosts192.168.162.30 harbor<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h1 id="九、配置阿里云的yum源"><a href="#九、配置阿里云的yum源" class="headerlink" title="九、配置阿里云的yum源"></a>九、配置阿里云的yum源</h1><p>epel工具源：<a href="https://developer.aliyun.com/mirror/epel?spm=a2c6h.13651102.0.0.3e221b11oIlZCb">https://developer.aliyun.com/mirror/epel?spm=a2c6h.13651102.0.0.3e221b11oIlZCb</a></p><p>centos阿里云源：<a href="https://developer.aliyun.com/mirror/centos?spm=a2c6h.13651102.0.0.3e221b11oIlZCb">https://developer.aliyun.com/mirror/centos?spm=a2c6h.13651102.0.0.3e221b11oIlZCb</a></p><pre class="line-numbers language-hljs plaintext"><code class="language-hljs plaintext">mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup# centos-8配置wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-vault-8.5.2111.repo# centos-7配置wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo# 刷新yum源 yum clean all && yum makecache<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一、Dockers核心技术精讲&quot;&gt;&lt;a href=&quot;#一、Dockers核心技术精讲&quot; class=&quot;headerlink&quot; title=&quot;一、Dockers核心技术精讲&quot;&gt;&lt;/a&gt;一、Dockers核心技术精讲&lt;/h1&gt;&lt;p&gt;虚拟化起源史：&lt;/p&gt;
&lt;p&gt;​	&lt;</summary>
      
    
    
    
    <category term="docker" scheme="http://kjiang.com/categories/docker/"/>
    
    
    <category term="docker" scheme="http://kjiang.com/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>Hello-World2</title>
    <link href="http://kjiang.com/posts/4181736637.html"/>
    <id>http://kjiang.com/posts/4181736637.html</id>
    <published>2025-03-11T06:39:04.000Z</published>
    <updated>2025-03-11T07:48:34.536Z</updated>
    
    <content type="html"><![CDATA[<h1 id="这是⼀篇加密文章"><a href="#这是⼀篇加密文章" class="headerlink" title="这是⼀篇加密文章"></a>这是⼀篇加密文章</h1><p> 这是文章内容</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;这是⼀篇加密文章&quot;&gt;&lt;a href=&quot;#这是⼀篇加密文章&quot; class=&quot;headerlink&quot; title=&quot;这是⼀篇加密文章&quot;&gt;&lt;/a&gt;这是⼀篇加密文章&lt;/h1&gt;&lt;p&gt; 这是文章内容&lt;/p&gt;
</summary>
      
    
    
    
    <category term="Markdown" scheme="http://kjiang.com/categories/Markdown/"/>
    
    
    <category term="Typora" scheme="http://kjiang.com/tags/Typora/"/>
    
    <category term="Markdown" scheme="http://kjiang.com/tags/Markdown/"/>
    
  </entry>
  
</feed>
